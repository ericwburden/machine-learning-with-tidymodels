[["index.html", "Machine Learning with TidyModels Chapter 1 About 1.1 Software Used", " Machine Learning with TidyModels Eric Burden 2022-10-19 Chapter 1 About Machine Learning with TidyModels is the result of my desire to gain a foundational understanding of machine learning and machine learning techniques without overcomplicating the process by wrestling with the huge variety of models and modeling packages available in the R ecosystem. The promise of the tidymodels meta-package is a unified interface for preparing data, training models, and leveraging the results. I find this to be an eminently sensible approach and see in tidymodels an opportunity to let the software “get out of the way” and let me focus on the ML algorithms themselves. For me, this book serves as a training exercise. I hope that, when finished, it will serve as a valuable reference. 1.1 Software Used This book was developed with the following software versions: R Language: 4.2.1 tidymodels: 1.0.0 tidyverse: 1.3.2 "],["the-tidymodels-workflow.html", "Chapter 2 The Tidymodels Workflow 2.1 About Tidymodels 2.2 Philosophy in Practice 2.3 Workflow Steps", " Chapter 2 The Tidymodels Workflow 2.1 About Tidymodels tidymodels is an R meta-package, meaning it is essentially a bundle of R packages organized around a common theme. In this case, the theme is “modeling and machine learning using tidyverse principles”. In order to make sense of that, one needs to have a general understanding of what the tidyverse is and its philosophy. The tidyverse is yet another R meta-package organized around the theme of “data science”. The tidyverse packages share a common design philosophy, grammar, and many underlying data structures, such that working with tidyverse packages feels to the user as if they are leveraging a single, wide-ranging library by a single author (in the best case). There is an entire book on the subject of using the tidyverse to perform common data science tasks(Wickham and Grolemund 2017), but for the purposes of this book, it is enough to understand the four principles upon which the tidyverse is built, that is(Wickham 2022): Re-use existing data structures Compose simple functions with the pipe Embrace functional programming Design for humans 2.1.1 Re-use existing data structures Where possible, tidyverse and, by extension, tidymodels packages rely on common data structures, typically a tibble or data.frame (a tibble is essentially an improved data.frame with much of the same API). For more focused operations on single data types, this is more often a standard R vector or an S3 object that shares the vector API. In short, data structures produced and consumed by tidyverse and tidymodels packages should behave as expected by someone who is familiar with base-R data structures. Data frames should be “tidy”, consisting of variables in columns and observations in rows(Wickham 2014). 2.1.2 Compose simple functions with pipes The magrittr library(Bache and Wickham 2022) has long provided a “pipe” (%&gt;%) operator in R. More recently, a pipe operator (|&gt;) has been introduced to the language itself. There are some subtle differences in usage between the two, but that is beyond the scope of this work. In an effort to be as forward-looking as possible, examples in this book will use the native pipe operator. Regardless of which operator is chosen, however, the effect is similar to the pipe operator (|) that many will be familiar with from the Unix shell, in that the pipe serves as an infix operator that takes the result of calling the left-hand side and passes that result as the first argument to the operation (usually a function call) on the right-hand side. This makes the examples below equivalent: output &lt;- paste(unlist(strsplit(&quot;Replace:colons:with:spaces&quot;, &quot;:&quot;)), collapse = &quot; &quot;) input_str &lt;- &quot;Replace:colons:with:spaces&quot; split_input &lt;- strsplit(input_str, &quot;:&quot;) split_input_vec &lt;- unlist(split_input) output &lt;- paste(split_input_vec, collapse = &quot; &quot;) output &lt;- &quot;Replace:colons:with:spaces&quot; |&gt; strsplit(&quot;:&quot;) |&gt; unlist() |&gt; paste(collapse = &quot; &quot;) output &lt;- &quot;Replace:colons:with:spaces&quot; |&gt; strsplit(&quot;:&quot;) |&gt; unlist() |&gt; paste(collapse = &quot; &quot;) (output &lt;- &quot;Replace:colons:with:spaces&quot; |&gt; strsplit(&quot;:&quot;) |&gt; unlist() |&gt; paste(collapse = &quot; &quot;)) ## [1] &quot;Replace colons with spaces&quot; The first and most obvious benefit of this approach is readability. Using the pipe syntax, operations can be written in the order they are performed, as opposed to the “inside-out” approach of the first example. We can also avoid cluttering up the R environment with as in the “intermediate variables” approach of the second example. Less obvious are the benefits to coding habits; effective use of the pipe operator encourages sensible function naming and organization. The trade-off is that the programmer needs to be careful not to take multiple piped operations too far, breaking up long series of pipes with some intermediate variables as appropriate. 2.1.3 Embrace functional programming There’s a lot to say about what functional programming actually is, but in this context it mostly means: immutable objects and copy-on-modify semantics, the frequent use of “pure” functions that do not have side-effects, using generic functions where possible and appropriate, and abstracting over loops in a more “iterator-like” fashion. 2.1.4 Design for humans This is a bit subjective, but the tidyverse and tidymodels strive to provide APIs that are intuitive for human users and friendly to the IDE such that autocomplete is a helpful tool for discovering functionality. Generally, being autocomplete-friendly means grouping function names under common prefixes like [str_subset, str_detect, str_extract]and [add_model, add_recipe, add_formula] as opposed to alternatives like [subset_str, detect_str, extract_str] and [model_add, recipe_add, forumal_add]or the like. 2.2 Philosophy in Practice These pieces of tidyverse philosophy adopted by tidymodels yield a common way of working with and thinking about code written using the tidymodels collection of packages. The most prominent example of this is the tidymodels workflow, or, rather, the workflow package which is bundled into tidymodels. Throughout this book, the term workflow object will be used to refer to an R object to which various pre-processing, modeling, and post-processing steps can be added to facilitate repeatable and ergonomic ML code. Commonly, this will consist of splitting the data (using the rsample package), creating and adding a recipe (from the recipes package), creating and adding a model (from the parsnip package), fitting the model, then using the model to make inferences or predictions. The recipe and model steps may be fairly dense as well, but every step along the way should ideally reflect the principles described above. This approach can be better demonstrated than described, as show below: # Split the data into testing and training steps mtcars_split &lt;- initial_split(mtcars) # Pre-process the data using a recipe (example_recipe &lt;- recipe(mpg ~ cyl + disp + hp + wt, data = training(mtcars_split)) |&gt; step_mutate_at(cyl, fn = factor) |&gt; step_log(disp, hp) |&gt; step_interact(terms = ~ disp:hp)) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Operations: ## ## Variable mutation for cyl ## Log transformation on disp, hp ## Interactions with disp:hp # Setup up the model (example_model &lt;- linear_reg() |&gt; set_engine(&quot;glm&quot;) |&gt; set_mode(&quot;regression&quot;)) ## Linear Regression Model Specification (regression) ## ## Computational engine: glm # Bundle the recipe and model into a workflow, fit the model (example_workflow &lt;- workflow() |&gt; add_recipe(example_recipe) |&gt; add_model(example_model) |&gt; fit(data = training(mtcars_split))) ## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────── ## 3 Recipe Steps ## ## • step_mutate_at() ## • step_log() ## • step_interact() ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────── ## ## Call: stats::glm(formula = ..y ~ ., family = stats::gaussian, data = data) ## ## Coefficients: ## (Intercept) cyl6 cyl8 disp hp wt disp_x_hp ## 89.680 -3.679 -4.521 -10.369 -8.644 -1.836 1.405 ## ## Degrees of Freedom: 23 Total (i.e. Null); 17 Residual ## Null Deviance: 1060 ## Residual Deviance: 69.29 AIC: 109.6 # Add predictions to training data example_predictions &lt;- augment(example_workflow, testing(mtcars_split)) # Check performance metrics(example_predictions, mpg, .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 3.50 ## 2 rsq standard 0.675 ## 3 mae standard 3.03 To the untrained eye, it may appear that all the functions used in this code are from a common library, given the similarity in their usage and the ease with which the outputs of one operation are passed along to the next, but there are actually at least five different R libraries at play here! This is a key benefit of tidymodels, the bundled packages work seamlessly together to provide a consistent developer experience. 2.3 Workflow Steps The typical steps involved in using tidymodels to train and work with ML models are described below. These steps may be undertaken in a different order than listed here and individual steps may not be as cleanly separated as indicated. There may be additional steps for training and testing multiple models or other complex tasks, but the following is a good, general guide. 2.3.1 Data Splitting In the first step, you’ll primary use the rsample (Silge et al. 2022) package to divide up the data for training, validating, and testing the model. At a minimum, this generally involves splitting the data set into training and testing samples. Models, sometimes many different models, can be trained on the training sample, but the testing sample should be used only once, to validate and measure model performance on “real” data at the end. You can also set up your data for re-sampling in this step. 2.3.2 Data Preparation The next step consists of creating a specification for data pre-processing using the recipes (Kuhn and Wickham 2022) package. This can include a number of steps, including specifying the relationships between the response and predictor variables, data cleaning, transforming predictors, etc. 2.3.3 Model Specification In this step, the parsnip (Kuhn and Vaughan 2022) package is typically used to create a model specification. This includes indicating the type of the model, the model engine, the model mode, and providing relevant parameters. 2.3.4 Bundling a Workflow This step utilizes the workflows (Vaughan and Couch 2022) package to bundle together the recipe and model specifications into a single workflow object, which can then be used to fit the model, make predictions, or other operations. 2.3.5 Checking Results In this step, the yardstick (Kuhn, Vaughan, and Hvitfeldt 2022) package is used to measure model performance. References "],["simple-linear-regression.html", "Chapter 3 Simple Linear Regression 3.1 Description 3.2 How it Works 3.3 Evaluating Validity 3.4 Evaluating Fit 3.5 Example", " Chapter 3 Simple Linear Regression 3.1 Description An approach for predicting a dependent (response) value \\(Y\\) based on a single independent (predictor) value \\(X\\), given that there is some proportional linear relationship between \\(X\\) and \\(Y\\). That is, assuming that one unit of change in \\(X\\) results in a consistent change (increase or decrease) in \\(Y\\). \\[Y \\approx \\beta_0 + \\beta_1X\\] In this equation, \\(\\beta_0\\) represents a constant offset (the intercept) and \\(\\beta_1\\) represents the amount \\(Y\\) changes for each change in \\(X\\) (the slope). In a Simple Linear Regression model, the goal is to estimate \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) within the observed range of \\(X\\) and \\(Y\\) values such that the predicted values of \\(Y\\) are as close to the actual \\(Y\\) values as possible, generally determined by minimizing the Residual Sum of Squares.1 3.2 How it Works Given that the goal is to estimate the slope and intercept of the linear formula representation and that we have a series of \\(X/Y\\) observations, the values of \\(\\beta_0\\) and \\(\\beta_1\\) can be estimated via \\[\\hat\\beta_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\] \\[\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1\\bar{x}\\] In code, this looks like: x &lt;- mtcars$hp # horsepower y &lt;- mtcars$mpg # fuel efficiency, in miles/gallon mean_diff &lt;- \\(x) x - mean(x) slope &lt;- sum(mean_diff(x) * mean_diff(y))/sum(mean_diff(x)^2) intercept &lt;- mean(y) - slope * mean(x) # Visualization. The line demonstrates the simple # linear regression fit as calculated above. ggplot(mtcars, aes(hp, mpg)) + geom_point(color = &quot;blue&quot;) + geom_abline(slope = slope, intercept = intercept, color = &quot;orange&quot;) + annotate( &quot;text&quot;, x = 300, y = 12.5, label = glue(&quot;Slope: {round(slope, 2)}\\nIntercept: {round(intercept, 2)}&quot;) ) + labs( title = &quot;Simple Linear Regression&quot;, x = &quot;Horsepower&quot;, y = &quot;Miles/Gallon&quot; ) + theme_minimal() So, for a single predictor variable, slope and intercept can be estimated by a relatively straightforward algorithm. 3.3 Evaluating Validity It is possible to estimate the Standard Error2 ([[Definitions#^72c250|definition]]) of a model using the formulae below: \\[e_1 = y_1 - \\hat\\beta_0 - \\hat\\beta_1x_1\\] \\[RSS = e_1^2 + e_2^2 + ... + e_n^2\\] \\[\\sigma \\approx RSE = \\sqrt{RSS/(n-2)}\\] \\[\\widehat{SE}(\\hat\\beta_0)^2 = \\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}}\\right]\\] \\[\\widehat{SE}(\\hat\\beta_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}}\\] In code, that looks like: predicted &lt;- (slope * x) + intercept residuals &lt;- predicted - y rss &lt;- sum(residuals^2) # residual sum of squares n &lt;- length(x) # number of observations rse &lt;- sqrt(rss / (n_observed - 2)) # residual standard error se_intcpt &lt;- sqrt(rse^2 * ((1/n) + (mean(x)^2 / sum((x - mean(x))^2)))) se_slope &lt;- sqrt(rse^2 / sum((x - mean(x))^2)) The 95% confidence interval (for the slope, in this example) can be given by: \\[\\hat\\beta_1 \\pm 2 \\times SE(\\hat\\beta_1)\\] From the above example, it looks like this in code: min_interval &lt;- slope - (2 * se_slope) max_interval &lt;- slope + (2 * se_slope) (conf_interval &lt;- c(min_interval, max_interval)) ## [1] -0.08846689 -0.04798967 We find that we can say with 95% confidence that the slope lies somewhere between -0.0884669 and -0.0479897, which is reasonably close to our input value. Is this enough to determine whether our \\(X\\) and \\(Y\\) are related? For that, we can calculate the t-statistic3 by: \\[t = \\frac{\\hat\\beta_1 - 0}{SE(\\hat\\beta_1)}\\] to measure the number of standard deviations that \\(\\hat\\beta_1\\) is away from 0. This t-statistic is further used to compute the p-value4. In general, small p-values indicate that it is unlikely to observe a substantial association between the predictor and response due to random chance, as opposed to a real association. While it is possible to easily calculate the t-statistic, finding the p-value generally requires a lookup in a t-distribution table. Thankfully, R can handle that bit for you, as we’ll see in the tidymodels example for this modeling approach. (t_statistic &lt;- (slope - 0)/se_slope) ## [1] -6.742389 The important part here is that a low p-value indicates that your model fit is, actually, applicable for the data you have applied the model to, which is ultimately the part we are interested in. 3.4 Evaluating Fit In the previous section, we saw reference to \\(e\\) as the difference between the observed and predicted response value for a given predictor value. For real data that can be fit by a linear regression, it’s better to assume that the actual formula is more like \\[ Y \\approx \\beta_0 + \\beta_1X + \\epsilon\\] where \\(\\epsilon\\) represents an “error term”, that is, the accumulation of all the unknown variables and errors that are affecting the relationship between \\(X\\) and \\(Y\\). This accounts, in a sort of hand-waving way, for differences between the predicted linear relationship and observed results. The quality of a linear regression fit, which is how well the slope and intercept have been estimated, can be determined from the residual standard error (which we calculated previously) and the \\(R^2\\) statistic. Of the two, the \\(R^2\\) statistic is more useful and easily understood. 3.4.1 Residual Standard Error The residual standard error (RSE), that is, the standard error for the observed residuals from the linear regression fit, is an estimate of the standard deviation of the error term above. It can be calculated via: \\[e_1 = y_1 - \\hat\\beta_0 - \\hat\\beta_1x_1\\] \\[RSS = e_1^2 + e_2^2 + ... + e_n^2\\] \\[\\sigma \\approx RSE = \\sqrt{RSS/(n-2)}\\] In general, a smaller RSE indicates a better fit of the data. RSE, however, is an absolute measure of the lack of fit measured in the same units as \\(Y\\), indicating how far off any given predicted \\(Y\\) value may be. 3.4.2 \\(R^2\\) Statistic Contrasted to RSE, \\(R^2\\) represents the proportion of variance explained by the error term and is always a value between 0 and 1. It can be calculated by: \\[TSS = \\sum{(y_i - \\bar{y})^2}\\] \\[R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\\] where TSS is the total sum of squares of the response values and RSS is the residual sum of squares (discussed above). TSS is a measure of the variability in \\(Y\\), whereas RSS is a measure of the variability that is left unexplained after performing the regression. In summary, this means that \\(R^2\\) measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\). Thus, an \\(R^2\\) close to 1 indicates a linear regression fit where most of the variability in the response is explained by the regression. 3.5 Example Now, with an understanding of the underlying math and relevant statistics, we can fit a simple linear model to the example dataset we’ve used thus far, that is: estimating the fuel efficiency of a vehicle from its horsepower rating. # In this example, we&#39;ll forgo the training/testing split to # be consistent with our calculated model above. # Pre-process the data using a simple, no-frills recipe (simple_linear_recipe &lt;- recipe(mpg ~ hp, data = mtcars)) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 # Setup up the model (simple_linear_model &lt;- linear_reg() |&gt; set_engine(&quot;lm&quot;) |&gt; set_mode(&quot;regression&quot;)) ## Linear Regression Model Specification (regression) ## ## Computational engine: lm # Bundle the recipe and model into a workflow, fit the model (simple_linear_workflow &lt;- workflow() |&gt; add_recipe(simple_linear_recipe) |&gt; add_model(simple_linear_model) |&gt; fit(data = mtcars)) ## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────── ## 0 Recipe Steps ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────── ## ## Call: ## stats::lm(formula = ..y ~ ., data = data) ## ## Coefficients: ## (Intercept) hp ## 30.09886 -0.06823 # Add predictions to training data simple_linear_predictions &lt;- augment(simple_linear_workflow, mtcars) # Evaluate validity tidy(simple_linear_workflow) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 30.1 1.63 18.4 6.64e-18 ## 2 hp -0.0682 0.0101 -6.74 1.79e- 7 The tidy function (from the broom (Robinson, Hayes, and Couch 2022) package, also included in tidymodels) provides a variety of coefficients related to the model, including the relevant terms, their estimates, and the p-value. In this case, the p-values for both the slope (of the hp term, which is the only predictor variable) and the intercept are both very small, indicating that a simple linear model is valid for making predictions on this data set. But, how good is the fit? # Evaluate fit metrics(simple_linear_predictions, mpg, .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 3.74 ## 2 rsq standard 0.602 ## 3 mae standard 2.91 By default, yardstick provides three metrics for a simple linear regression model. rmse is Root-Mean-Square Error, mae is Mean Absolute Error, and rsq is \\(R^2\\), our preferred metric for determining model fit. References "],["references.html", "Chapter 4 References", " Chapter 4 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
