[["index.html", "Machine Learning with TidyModels Chapter 1 About 1.1 Software Used", " Machine Learning with TidyModels Eric Burden 2022-11-06 Chapter 1 About Machine Learning with TidyModels is the result of my desire to gain a foundational understanding of machine learning and machine learning techniques without overcomplicating the process by wrestling with the huge variety of models and modeling packages available in the R ecosystem. The promise of the tidymodels meta-package is a unified interface for preparing data, training models, and leveraging the results. I find this to be an eminently sensible approach and see in tidymodels an opportunity to let the software “get out of the way” and let me focus on the ML algorithms themselves. For me, this book serves as a training exercise. I hope that, when finished, it will serve as a valuable reference. 1.1 Software Used This book was developed with the following software versions: R Language: 4.2.1 tidymodels: 1.0.0 tidyverse: 1.3.2 "],["chapter-2.html", "Chapter 2 The Tidymodels Workflow 2.1 About Tidymodels 2.2 Philosophy in Practice 2.3 Workflow Steps", " Chapter 2 The Tidymodels Workflow 2.1 About Tidymodels tidymodels is an R meta-package, meaning it is essentially a bundle of R packages organized around a common theme. In this case, the theme is “modeling and machine learning using tidyverse principles”. In order to make sense of that, one needs to have a general understanding of what the tidyverse is and its philosophy. The tidyverse is yet another R meta-package organized around the theme of “data science”. The tidyverse packages share a common design philosophy, grammar, and many underlying data structures, such that working with tidyverse packages feels to the user as if they are leveraging a single, wide-ranging library by a single author (in the best case). There is an entire book on the subject of using the tidyverse to perform common data science tasks(Wickham and Grolemund 2017), but for the purposes of this book, it is enough to understand the four principles upon which the tidyverse is built, that is(Wickham 2022): Re-use existing data structures Compose simple functions with the pipe Embrace functional programming Design for humans 2.1.1 Re-use existing data structures Where possible, tidyverse and, by extension, tidymodels packages rely on common data structures, typically a tibble or data.frame (a tibble is essentially an improved data.frame with much of the same API). For more focused operations on single data types, this is more often a standard R vector or an S3 object that shares the vector API. In short, data structures produced and consumed by tidyverse and tidymodels packages should behave as expected by someone who is familiar with base-R data structures. Data frames should be “tidy”, consisting of variables in columns and observations in rows(Wickham 2014). 2.1.2 Compose simple functions with pipes The magrittr library(Bache and Wickham 2022) has long provided a “pipe” (%&gt;%) operator in R. More recently, a pipe operator (|&gt;) has been introduced to the language itself. There are some subtle differences in usage between the two, but that is beyond the scope of this work. In an effort to be as forward-looking as possible, examples in this book will use the native pipe operator. Regardless of which operator is chosen, however, the effect is similar to the pipe operator (|) that many will be familiar with from the Unix shell, in that the pipe serves as an infix operator that takes the result of calling the left-hand side and passes that result as the first argument to the operation (usually a function call) on the right-hand side. This makes the examples below equivalent: output &lt;- paste(unlist(strsplit(&quot;Replace:colons:with:spaces&quot;, &quot;:&quot;)), collapse = &quot; &quot;) input_str &lt;- &quot;Replace:colons:with:spaces&quot; split_input &lt;- strsplit(input_str, &quot;:&quot;) split_input_vec &lt;- unlist(split_input) output &lt;- paste(split_input_vec, collapse = &quot; &quot;) output &lt;- &quot;Replace:colons:with:spaces&quot; |&gt; strsplit(&quot;:&quot;) |&gt; unlist() |&gt; paste(collapse = &quot; &quot;) output &lt;- &quot;Replace:colons:with:spaces&quot; |&gt; strsplit(&quot;:&quot;) |&gt; unlist() |&gt; paste(collapse = &quot; &quot;) (output &lt;- &quot;Replace:colons:with:spaces&quot; |&gt; strsplit(&quot;:&quot;) |&gt; unlist() |&gt; paste(collapse = &quot; &quot;)) ## [1] &quot;Replace colons with spaces&quot; The first and most obvious benefit of this approach is readability. Using the pipe syntax, operations can be written in the order they are performed, as opposed to the “inside-out” approach of the first example. We can also avoid cluttering up the R environment with as in the “intermediate variables” approach of the second example. Less obvious are the benefits to coding habits; effective use of the pipe operator encourages sensible function naming and organization. The trade-off is that the programmer needs to be careful not to take multiple piped operations too far, breaking up long series of pipes with some intermediate variables as appropriate. 2.1.3 Embrace functional programming There’s a lot to say about what functional programming actually is, but in this context it mostly means: immutable objects and copy-on-modify semantics, the frequent use of “pure” functions that do not have side-effects, using generic functions where possible and appropriate, and abstracting over loops in a more “iterator-like” fashion. 2.1.4 Design for humans This is a bit subjective, but the tidyverse and tidymodels strive to provide APIs that are intuitive for human users and friendly to the IDE such that autocomplete is a helpful tool for discovering functionality. Generally, being autocomplete-friendly means grouping function names under common prefixes like [str_subset, str_detect, str_extract]and [add_model, add_recipe, add_formula] as opposed to alternatives like [subset_str, detect_str, extract_str] and [model_add, recipe_add, forumal_add]or the like. 2.2 Philosophy in Practice These pieces of tidyverse philosophy adopted by tidymodels yield a common way of working with and thinking about code written using the tidymodels collection of packages. The most prominent example of this is the tidymodels workflow, or, rather, the workflow package which is bundled into tidymodels. Throughout this book, the term workflow object will be used to refer to an R object to which various pre-processing, modeling, and post-processing steps can be added to facilitate repeatable and ergonomic ML code. Commonly, this will consist of splitting the data (using the rsample package), creating and adding a recipe (from the recipes package), creating and adding a model (from the parsnip package), fitting the model, then using the model to make inferences or predictions. The recipe and model steps may be fairly dense as well, but every step along the way should ideally reflect the principles described above. This approach can be better demonstrated than described, as show below: # Split the data into testing and training steps mtcars_split &lt;- initial_split(mtcars) # Pre-process the data using a recipe (example_recipe &lt;- recipe(mpg ~ cyl + disp + hp + wt, data = training(mtcars_split)) |&gt; step_mutate_at(cyl, fn = factor) |&gt; step_log(disp, hp) |&gt; step_interact(terms = ~ disp:hp)) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Operations: ## ## Variable mutation for cyl ## Log transformation on disp, hp ## Interactions with disp:hp # Setup up the model (example_model &lt;- linear_reg() |&gt; set_engine(&quot;glm&quot;) |&gt; set_mode(&quot;regression&quot;)) ## Linear Regression Model Specification (regression) ## ## Computational engine: glm # Bundle the recipe and model into a workflow, fit the model (example_workflow &lt;- workflow() |&gt; add_recipe(example_recipe) |&gt; add_model(example_model) |&gt; fit(data = training(mtcars_split))) ## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────── ## 3 Recipe Steps ## ## • step_mutate_at() ## • step_log() ## • step_interact() ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────── ## ## Call: stats::glm(formula = ..y ~ ., family = stats::gaussian, data = data) ## ## Coefficients: ## (Intercept) cyl6 cyl8 disp hp wt disp_x_hp ## 116.9581 -0.9030 0.5044 -11.9599 -16.0951 -2.5162 2.0495 ## ## Degrees of Freedom: 23 Total (i.e. Null); 17 Residual ## Null Deviance: 904.6 ## Residual Deviance: 95.07 AIC: 117.1 # Add predictions to training data example_predictions &lt;- augment(example_workflow, testing(mtcars_split)) # Check performance metrics(example_predictions, mpg, .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 2.06 ## 2 rsq standard 0.861 ## 3 mae standard 1.62 To the untrained eye, it may appear that all the functions used in this code are from a common library, given the similarity in their usage and the ease with which the outputs of one operation are passed along to the next, but there are actually at least five different R libraries at play here! This is a key benefit of tidymodels, the bundled packages work seamlessly together to provide a consistent developer experience. 2.3 Workflow Steps The typical steps involved in using tidymodels to train and work with ML models are described below. These steps may be undertaken in a different order than listed here and individual steps may not be as cleanly separated as indicated. There may be additional steps for training and testing multiple models or other complex tasks, but the following is a good, general guide. 2.3.1 Data Splitting In the first step, you’ll primary use the rsample (Silge et al. 2022) package to divide up the data for training, validating, and testing the model. At a minimum, this generally involves splitting the data set into training and testing samples. Models, sometimes many different models, can be trained on the training sample, but the testing sample should be used only once, to validate and measure model performance on “real” data at the end. You can also set up your data for re-sampling in this step. 2.3.2 Data Preparation The next step consists of creating a specification for data pre-processing using the recipes (Kuhn and Wickham 2022) package. This can include a number of steps, including specifying the relationships between the response and predictor variables, data cleaning, transforming predictors, etc. 2.3.3 Model Specification In this step, the parsnip (Kuhn and Vaughan 2022) package is typically used to create a model specification. This includes indicating the type of the model, the model engine, the model mode, and providing relevant parameters. 2.3.4 Bundling a Workflow This step utilizes the workflows (Vaughan and Couch 2022) package to bundle together the recipe and model specifications into a single workflow object, which can then be used to fit the model, make predictions, or other operations. 2.3.5 Checking Results In this step, the yardstick (Kuhn, Vaughan, and Hvitfeldt 2022) package is used to measure model performance. References "],["chapter-3.html", "Chapter 3 Simple Linear Regression 3.1 Description 3.2 How it Works 3.3 Evaluating Validity 3.4 Evaluating Fit 3.5 Example 3.6 Considerations", " Chapter 3 Simple Linear Regression 3.1 Description An approach for predicting a dependent (response) value \\(Y\\) based on a single independent (predictor) value \\(X\\), given that there is some proportional linear relationship between \\(X\\) and \\(Y\\). That is, assuming that one unit of change in \\(X\\) results in a consistent change (increase or decrease) in \\(Y\\). \\[Y \\approx \\beta_0 + \\beta_1X\\] In this equation, \\(\\beta_0\\) represents a constant offset (the intercept) and \\(\\beta_1\\) represents the amount \\(Y\\) changes for each change in \\(X\\) (the slope). In a Simple Linear Regression model, the goal is to estimate \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) within the observed range of \\(X\\) and \\(Y\\) values such that the predicted values of \\(Y\\) are as close to the actual \\(Y\\) values as possible, generally determined by minimizing the Residual Sum of Squares.1 3.2 How it Works Given that the goal is to estimate the slope and intercept of the linear formula representation and that we have a series of \\(X/Y\\) observations, the values of \\(\\beta_0\\) and \\(\\beta_1\\) can be estimated via \\[\\hat\\beta_1 = \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\] \\[\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1\\bar{x}\\] In code, this looks like: x &lt;- mtcars$hp # horsepower y &lt;- mtcars$mpg # fuel efficiency, in miles/gallon mean_diff &lt;- \\(x) x - mean(x) slope &lt;- sum(mean_diff(x) * mean_diff(y))/sum(mean_diff(x)^2) intercept &lt;- mean(y) - slope * mean(x) # Visualization. The line demonstrates the simple # linear regression fit as calculated above. ggplot(mtcars, aes(hp, mpg)) + geom_point(color = &quot;blue&quot;) + geom_abline(slope = slope, intercept = intercept, color = &quot;orange&quot;) + annotate( &quot;text&quot;, x = 300, y = 12.5, label = glue(&quot;Slope: {round(slope, 2)}\\nIntercept: {round(intercept, 2)}&quot;) ) + labs( title = &quot;Simple Linear Regression&quot;, x = &quot;Horsepower&quot;, y = &quot;Miles/Gallon&quot; ) + theme_minimal() So, for a single predictor variable, slope and intercept can be estimated by a relatively straightforward algorithm. 3.3 Evaluating Validity It is possible to estimate the Standard Error2 ([[Definitions#^72c250|definition]]) of a model using the formulae below: \\[e_1 = y_1 - \\hat\\beta_0 - \\hat\\beta_1x_1\\] \\[RSS = e_1^2 + e_2^2 + ... + e_n^2\\] \\[\\sigma \\approx RSE = \\sqrt{RSS/(n-2)}\\] \\[\\widehat{SE}(\\hat\\beta_0)^2 = \\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}}\\right]\\] \\[\\widehat{SE}(\\hat\\beta_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}}\\] In code, that looks like: predicted &lt;- (slope * x) + intercept residuals &lt;- predicted - y rss &lt;- sum(residuals^2) # residual sum of squares n &lt;- length(x) # number of observations rse &lt;- sqrt(rss / (n_observed - 2)) # residual standard error se_intcpt &lt;- sqrt(rse^2 * ((1/n) + (mean(x)^2 / sum((x - mean(x))^2)))) se_slope &lt;- sqrt(rse^2 / sum((x - mean(x))^2)) The 95% confidence interval (for the slope, in this example) can be given by: \\[\\hat\\beta_1 \\pm 2 \\times SE(\\hat\\beta_1)\\] From the above example, it looks like this in code: min_interval &lt;- slope - (2 * se_slope) max_interval &lt;- slope + (2 * se_slope) (conf_interval &lt;- c(min_interval, max_interval)) ## [1] -0.08846689 -0.04798967 We find that we can say with 95% confidence that the slope lies somewhere between -0.0884669 and -0.0479897, which is reasonably close to our input value. Is this enough to determine whether our \\(X\\) and \\(Y\\) are related? For that, we can calculate the t-statistic3 by: \\[t = \\frac{\\hat\\beta_1 - 0}{SE(\\hat\\beta_1)}\\] to measure the number of standard deviations that \\(\\hat\\beta_1\\) is away from 0. This t-statistic is further used to compute the p-value4. In general, small p-values indicate that it is unlikely to observe a substantial association between the predictor and response due to random chance, as opposed to a real association. While it is possible to easily calculate the t-statistic, finding the p-value generally requires a lookup in a t-distribution table. Thankfully, R can handle that bit for you, as we’ll see in the tidymodels example for this modeling approach. (t_statistic &lt;- (slope - 0)/se_slope) ## [1] -6.742389 The important part here is that a low p-value indicates that your model fit is, actually, applicable for the data you have applied the model to, which is ultimately the part we are interested in. 3.4 Evaluating Fit In the previous section, we saw reference to \\(e\\) as the difference between the observed and predicted response value for a given predictor value. For real data that can be fit by a linear regression, it’s better to assume that the actual formula is more like \\[ Y \\approx \\beta_0 + \\beta_1X + \\epsilon\\] where \\(\\epsilon\\) represents an “error term”, that is, the accumulation of all the unknown variables and errors that are affecting the relationship between \\(X\\) and \\(Y\\). This accounts, in a sort of hand-waving way, for differences between the predicted linear relationship and observed results. The quality of a linear regression fit, which is how well the slope and intercept have been estimated, can be determined from the residual standard error (which we calculated previously) and the \\(R^2\\) statistic. Of the two, the \\(R^2\\) statistic is more useful and easily understood. 3.4.1 Residual Standard Error The residual standard error (RSE), that is, the standard error for the observed residuals from the linear regression fit, is an estimate of the standard deviation of the error term above. It can be calculated via: \\[e_1 = y_1 - \\hat\\beta_0 - \\hat\\beta_1x_1\\] \\[RSS = e_1^2 + e_2^2 + ... + e_n^2\\] \\[\\sigma \\approx RSE = \\sqrt{RSS/(n-2)}\\] In general, a smaller RSE indicates a better fit of the data. RSE, however, is an absolute measure of the lack of fit measured in the same units as \\(Y\\), indicating how far off any given predicted \\(Y\\) value may be. 3.4.2 \\(R^2\\) Statistic Contrasted to RSE, \\(R^2\\) represents the proportion of variance explained by the error term and is always a value between 0 and 1. It can be calculated by: \\[TSS = \\sum{(y_i - \\bar{y})^2}\\] \\[R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\\] where TSS is the total sum of squares of the response values and RSS is the residual sum of squares (discussed above). TSS is a measure of the variability in \\(Y\\), whereas RSS is a measure of the variability that is left unexplained after performing the regression. In summary, this means that \\(R^2\\) measures the proportion of variability in \\(Y\\) that can be explained using \\(X\\). Thus, an \\(R^2\\) close to 1 indicates a linear regression fit where most of the variability in the response is explained by the regression. 3.5 Example Now, with an understanding of the underlying math and relevant statistics, we can fit a simple linear model to the example dataset we’ve used thus far, that is: estimating the fuel efficiency of a vehicle from its horsepower rating. # In this example, we&#39;ll forgo the training/testing split to # be consistent with our calculated model above. # Pre-process the data using a simple, no-frills recipe (simple_linear_recipe &lt;- recipe(mpg ~ hp, data = mtcars)) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 # Setup up the model (simple_linear_model &lt;- linear_reg() |&gt; set_engine(&quot;lm&quot;) |&gt; set_mode(&quot;regression&quot;)) ## Linear Regression Model Specification (regression) ## ## Computational engine: lm # Bundle the recipe and model into a workflow, fit the model (simple_linear_workflow &lt;- workflow() |&gt; add_recipe(simple_linear_recipe) |&gt; add_model(simple_linear_model) |&gt; fit(data = mtcars)) ## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────── ## 0 Recipe Steps ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────── ## ## Call: ## stats::lm(formula = ..y ~ ., data = data) ## ## Coefficients: ## (Intercept) hp ## 30.09886 -0.06823 # Add predictions to training data simple_linear_predictions &lt;- augment(simple_linear_workflow, mtcars) # Evaluate validity tidy(simple_linear_workflow) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 30.1 1.63 18.4 6.64e-18 ## 2 hp -0.0682 0.0101 -6.74 1.79e- 7 The tidy function (from the broom (Robinson, Hayes, and Couch 2022) package, also included in tidymodels) provides a variety of coefficients related to the model, including the relevant terms, their estimates, and the p-value. In this case, the p-values for both the slope (of the hp term, which is the only predictor variable) and the intercept are both very small, indicating that a simple linear model is valid for making predictions on this data set. But, how good is the fit? # Evaluate fit metrics(simple_linear_predictions, mpg, .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 3.74 ## 2 rsq standard 0.602 ## 3 mae standard 2.91 By default, yardstick provides three metrics for a simple linear regression model. rmse is Root-Mean-Square Error, mae is Mean Absolute Error, and rsq is \\(R^2\\), our preferred metric for determining model fit. 3.6 Considerations 3.6.1 Limits of Applicability Predictions made using a linear regression model are most accurate when made within the ranges of the observed data used to build the model. This presents a difficulty when, for example, attempting to predict future results. Say, you want to understand what may happen to program participation (Y) if the budget for your program (X) is increased beyond previous levels. The further beyond the previous maximum you increase X, the less likely this model is to be accurate. 3.6.2 Outliers The presence of outliers, or observations with an unusual response value, in the data can have an outsized impact on quality measures like RSE and \\(R^2\\), while not necessarily having an equivalent impact on the fitted model parameters. Plotting studentized residuals can help reveal outliers. Typically, observations whose studentized residuals are greater than 3 should be examined and considered for exclusion. 3.6.3 High-leverage points Contrasted to outliers, high-leverage points have unusual predictor values. These values can have an outsized impact on the fit of the model. They can be detected by calculating the leverage statistic. If an observation has a leverage statistic that greatly exceeds \\((p+1)/n\\), then it may be a high-leverage point. Plotting the studentized residual vs the leverage statistic for each observation can be used to spot outliers and high leverage points simultaneously. References "],["chapter-4.html", "Chapter 4 Multiple Linear Regression 4.1 Description 4.2 How it Works 4.3 Evaluating Validity 4.4 Identifying Important Predictors 4.5 Evaluating Fit 4.6 Example 4.7 Considerations", " Chapter 4 Multiple Linear Regression 4.1 Description An approach for predicting a dependent (response) value \\(Y\\) based on multiple independent (predictor) values \\(X_1, X_2, X_3, ..., X_n\\), given that there is some proportional linear relationship between the predictor values and the response value. That is, assuming that one unit of change in any predictor value results in a consistent change (increase or decrease) in the response value. \\[ Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon\\] In this equation, just like for simple linear regression, \\(\\beta_0\\) represents a constant offset (the intercept) and \\(\\beta_i\\) represents the average amount \\(Y\\) changes for each change in \\(X_i\\) (the slope for that predictor) while holding all other predictor variables fixed. 4.2 How it Works Multiple Linear Regression behaves similarly to Simple Linear Regression, with the exception that instead of representing the fit via a 2-dimensional line, the dimensionality of the fit is one greater than the number of predictors. For example, the two-predictor case can be represented as a plane through a three-dimensional grid with \\(Y\\), \\(X_1\\), and \\(X_2\\) axes. The slopes and intercept for a multi-dimensional fit can be calculated similar to the Simple Linear model, but the math is apparently gnarly enough that ISLR declined to include it. 4.3 Evaluating Validity Instead of the t-statistic or p-value, the applicability of a Multiple Linear Regression model can be evaluated using the F-statistic, which can be calculated for n observations and p predictors as: \\[ \\begin{align} RSS &amp; = \\sum_{i=1}^n{(y_i - \\hat{y}_i)^2} \\\\ &amp; = \\sum_{i=1}^2{(y_1 - \\beta_0 - \\beta_1X_1 - \\beta_2X_2 - \\ ... \\ - \\beta_nX_n)} \\\\ TSS &amp; = \\sum_{i=1}^n{(y_i - \\bar{y})^2} \\\\ F &amp; = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} \\end{align} \\] If the assumptions of a linear model are correct, then: \\[E\\{RSS/(n - p - 1)\\} = \\sigma^2\\] If the null hypothesis is true, that is all the slopes of the model are 0, then: \\[E\\{(TSS - RSS)/p\\} = \\sigma^2\\] So, if there is no relationship between the response and predictors, we would expect the F-statistic to be near 1, since \\(\\frac{\\sigma^2}{\\sigma^2} = 1\\). On the other hand, if there is some relationship between the response and predictors, we expect the numerator of the F-statistic to be greater than \\(\\sigma^2\\), making \\(F\\) greater than 1. How large \\(F\\) needs to be to indicate the applicability of the model is dependent on the size of n and p. When the number of observations is large, then a smaller \\(F\\) can indicate the relationship. The p-value can be calculated from \\(F\\) for a given n and p, as well as for each predictor variable. These values are reported by the base R lm model, which can handle multiple linear regression. The code looks like: set.seed(1337) x1 &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) x2 &lt;- c(1, 3, 5, 7, 11, 13, 17, 23, 29, 31) y &lt;- 5 + 2*x1 + 0.33*x2 + runif(10, -0.5, 0.5) fit &lt;- lm(y ~ x1 + x2) summary(fit) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.39015 -0.10268 -0.04461 0.09799 0.51006 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.85564 0.29322 16.560 7.15e-07 *** ## x1 2.08968 0.15838 13.194 3.36e-06 *** ## x2 0.29789 0.04474 6.659 0.000288 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2669 on 7 degrees of freedom ## Multiple R-squared: 0.9994, Adjusted R-squared: 0.9992 ## F-statistic: 5681 on 2 and 7 DF, p-value: 5.79e-12 #&gt; Call: #&gt; lm(formula = y ~ x1 + x2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.39015 -0.10268 -0.04461 0.09799 0.51006 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.85564 0.29322 16.560 7.15e-07 *** #&gt; x1 2.08968 0.15838 13.194 3.36e-06 *** #&gt; x2 0.29789 0.04474 6.659 0.000288 *** #&gt; --- #&gt; Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #&gt; #&gt; Residual standard error: 0.2669 on 7 degrees of freedom #&gt; Multiple R-squared: 0.9994, Adjusted R-squared: 0.9992 #&gt; F-statistic: 5681 on 2 and 7 DF, p-value: 5.79e-12 Here we see a larger p-value for x1 compared to x2, which makes sense given that the slope for x2 is smaller. The overall p-value is also quite small and \\(F\\) is quite large, indicating that the response and predictors do have a relationship. Compare that to: y &lt;- runif(10, 0, 40) # random values fit &lt;- lm(y ~ x1 + x2) summary(fit) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.8573 -8.3492 0.6098 11.5581 19.5387 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 45.729 17.386 2.630 0.0339 * ## x1 -7.257 9.391 -0.773 0.4649 ## x2 1.437 2.653 0.542 0.6048 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.82 on 7 degrees of freedom ## Multiple R-squared: 0.22, Adjusted R-squared: -0.002895 ## F-statistic: 0.987 on 2 and 7 DF, p-value: 0.4192 #&gt; Call: #&gt; lm(formula = y ~ x1 + x2) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -19.8573 -8.3492 0.6098 11.5581 19.5387 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 45.729 17.386 2.630 0.0339 * #&gt; x1 -7.257 9.391 -0.773 0.4649 #&gt; x2 1.437 2.653 0.542 0.6048 #&gt; --- #&gt; Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #&gt; #&gt; Residual standard error: 15.82 on 7 degrees of freedom #&gt; Multiple R-squared: 0.22, Adjusted R-squared: -0.002895 #&gt; F-statistic: 0.987 on 2 and 7 DF, p-value: 0.4192 Here, the overall p-value is rather large, and \\(F\\) is very nearly 1, indicating (as we suspect) that y cannot be predicted by x1 and x2. 4.4 Identifying Important Predictors Even after determining that a group of predictors can accurately (to some degree) be used to model the behavior of the response value, it is entirely possible (even likely) that some predictors are more influential than others. Some predictors may even be superfluous. By removing predictors whose explanatory power is weak, the model can be refined and the model interpretation can be sharpened. For example, if we determine that weight, horsepower, and engine displacement all affect the MPG we can expect from a vehicle, and we want to improve fuel efficiency, then in the real world it is beneficial to determine which inputs have the biggest impact on the output so that we can focus our attention on those significant inputs. One can start by examining the individual p-values reported by the model, but recall that these p-values where calculated when all predictors were modeled. Adding or removing predictors may change the p-values for any or all of the remaining predictors (perhaps there are interaction effects not accounted for). One can imagine several strategies for selecting predictors, however, this approach is not recommended (or supported) in tidymodels. Instead, various regularization approaches are provided which penalize (or reduce the contributions of) less important variables. These methods will be covered in more detail in a later chapter. 4.5 Evaluating Fit RSE and \\(R^2\\) can be computed and interpreted the same as for Simple Linear models . For Multiple Linear Regression, \\(R^2\\) equals \\(Cor(Y,\\hat{Y})^2\\), which is the square of the correlation between the response variable and the predicted response variable. It is important to note that adding more predictor variables to the model will always increase \\(R^2\\), even if those predictor variables are only weakly associated with the response. This is due to over-fitting. Care should be taken when adding additional predictor variables to the model to ensure that each is substantially contributing to the fit. This can be assessed by observing the effect on RSE and \\(R^2\\) of adding or removing a particular predictor. If the impact is minimal, that predictor is likely unnecessary. RSE in particular can help evaluate this, as the RSE can increase slightly if the additional predictor variables only have a small impact on RSS (residual sum of squares). 4.6 Example Now that we have an underpinning for the way multiple predictor variables impact the linear regression approach, we can explore the tidymodels workflow for this approach. We can add exploratory data analysis steps to suss out any complicating issues (discussed in Considerations). 4.6.1 Correlated Predictors # Use a scatterplot matrix to identify correlations GGally::ggpairs(mtcars, columns = c(&quot;disp&quot;, &quot;hp&quot;, &quot;drat&quot;, &quot;wt&quot;, &quot;qsec&quot;), progress = F) In this scatterplot matrix, the bottom half of the plot shows scatterplots for each row/column combination, the top half includes the raw correlations and their significance (more asterisks == more correlation), and the central diagonal indicates a histogram of the relevant variable. We can see from this plot that there are some major predictor correlations here that we can account for in our model fit. Note also the left-biased histograms for disp and hp. This can generally be detrimental to the model fit and can be improved by log-transforming the predictor variable. This will be shown below. 4.6.2 Tidymodels Workflow set.seed(1234) # Note, we&#39;re still not splitting the dataset here, for the sake of # comparison to the Simple Linear Regression example. # Pre-process the data using a recipe # Note, this is slightly different than the example from Chapter 2, # here we add interaction terms for the many interactions observed. (multiple_linear_regression_recipe &lt;- recipe(mpg ~ disp + hp + drat + wt + qsec, data = mtcars) |&gt; step_log(disp, hp) |&gt; step_interact( terms = ~ disp:hp + disp:drat + disp:wt + hp:wt + hp:qsec + drat:wt) ) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 5 ## ## Operations: ## ## Log transformation on disp, hp ## Interactions with disp:hp + disp:drat + disp:wt + hp:wt + hp:qsec + drat:wt # Setup up the model (multiple_linear_regression_model &lt;- linear_reg() |&gt; set_engine(&quot;lm&quot;) |&gt; set_mode(&quot;regression&quot;)) ## Linear Regression Model Specification (regression) ## ## Computational engine: lm # Bundle the recipe and model into a workflow, fit the model (multiple_linear_regression_workflow &lt;- workflow() |&gt; add_recipe(multiple_linear_regression_recipe) |&gt; add_model(multiple_linear_regression_model) |&gt; fit(data = training(mtcars_split))) ## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────── ## 2 Recipe Steps ## ## • step_log() ## • step_interact() ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────── ## ## Call: ## stats::lm(formula = ..y ~ ., data = data) ## ## Coefficients: ## (Intercept) disp hp drat wt qsec disp_x_hp ## 9.8223 -5.0773 9.2242 21.5371 -26.5927 1.5736 1.9581 ## disp_x_drat disp_x_wt hp_x_wt hp_x_qsec drat_x_wt ## -6.8414 6.9204 -6.4707 -0.2865 5.1099 # Add predictions to training data multiple_linear_regression_predictions &lt;- augment( multiple_linear_regression_workflow, mtcars ) # Check performance (metrics(multiple_linear_regression_predictions, mpg, .pred)) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.91 ## 2 rsq standard 0.899 ## 3 mae standard 1.54 # It is often helpful to visualze results like this, comparing the # actual values to the predicted values of the response variable. (multiple_linear_regression_predictions |&gt; ggplot(aes(x = .pred, y = mpg)) + geom_point(color = &quot;blue&quot;) + geom_abline(intercept = 0, slope = 1, color = &quot;orange&quot;) + labs(title = &quot;Multiple Linear Regression&quot;, x = &quot;Predicted&quot;, y = &quot;Actual&quot;) + theme_minimal()) Looking at the performance metrics, we can see that the estimated fit of the multiple linear regression has a better estimated \\(R^2\\) than the simple linear regression model from Chapter 3. We shouldn’t be too overconfident here, though, adding additional variables always increases \\(R^2\\). The visualization reveals a good fit, as well. For real data, this test of performance would be conducted against a testing set, which we will see in future examples. 4.7 Considerations 4.7.1 Not Just Combining Simple Models When multiple predictor values are present, the Multiple Linear Regression model is vastly preferred over combining or averaging multiple Simple Linear Regression fits, as the Multiple Linear Regression may reveal interaction effects that will be masked by multiple Simple Linear Regression fits. This makes intuitive sense, as we can imagine real-life scenarios in which correlated observations are not necessarily related directly, but may have a single cause (i.e., increased ice cream sales do not cause an increase in shark attacks). 4.7.2 Qualitative Variables Qualitative predictor variables (aka categories, or factors) can be used as predictor variables as well, they simple need to be converted into quantitative dummy variables, where presence or absence of a category is denoted by a 1 or 0 in the column, respectively. In practice, this process is handled by R’s formula syntax and is not something that must be considered, beyond ensuring that qualitative variables are encoded as factors and not strings. 4.7.3 Predictor Interactions It is possible that each unit change in one predictor variable can impact not just the response variable but also the slope for another predictor variable. An example is the number of tools and the number of workers on the rate of production in some factory setting. It seems clear that both input variables are connected in some way, since an abundance of workers with very few pieces of equipment would produce a totally different output than a moderate number of both. These interaction effects can be accounted for by adding an interaction column to the data set (multiplying the two values together) or by adding interaction terms to the formula specification in R. For example, units ~ workers + equipment + workers:equipment, units ~ workers + equipment + workers*equipment, and units ~ workers*equipment are all equivalent and different spellings of the formula that includes contributions by workers, equipment, and the interaction between the two. 4.7.4 Polynomial Regression There are times when the relationship between a predictor and response is not a straight line, but can be described by a curve. In these cases, polynomial regression can be employed. One strategy to deal with this is to include powers of the predictor variables as additional predictors. For example, using the mtcars data set, it may be that mpg and horsepower have a quadratic relationship. This could be modeled by \\[mpg = \\beta_0 + (\\beta_1 \\times horsepower) + (\\beta_2 \\times horsepower^2) + \\epsilon\\] Where the square of horsepower is included as a predictor. This allows for the same linear regression to fit a curved line. The user should be careful to not over-use polynomials, however, as this is a potential source of over-fitting. 4.7.5 Potential Issues 4.7.5.1 Non-linearity of response-predictor relationships If the relationship between the response and predictors is truly not linear, then a linear regression model cannot be properly fit to the data. This issue can be diagnosed using a residual plot. When plotting the residuals vs the predicted response values, there should be no discernible pattern. If there is an observable pattern, then it indicates a systematic mis-fit of the model to the response. At times, this can be corrected by applying non-linear transformations to predictors such as \\(\\log{X}\\), \\(\\sqrt{X}\\), or \\(X^2\\). 4.7.5.2 Correlation of error terms Linear regression models assume that the error inherent in each measurement is independent. If this assumption is violated, then the model may overestimate the calculated confidence interval (i.e., indicate narrower confidence and prediction intervals than would reflect reality). This can easily occur with time series data, where sequential measurements are captured close in time. If the error terms are correlated, then the residuals from fitting this kind of data may demonstrate an phenomenon known as tracking, where adjacent residuals may have similar values. In these cases, linear regression may not be the right choice. 4.7.5.3 Non-constant variance of error terms Linear regression models assume that the error terms have constant variance, that is, they fall within the same distribution and limits for each response measurement. A non-constant variance could be expressed if the error ranges become larger at larger response values, for example. This is called heteroscedasticity. It can be observed in residual plots as well, often as a “funnel” shape. It can be corrected by applying a “concave” transformation to one or more predictors, such as \\(\\log{X}\\) or \\(\\sqrt{X}\\). Another option, especially if you have a good idea as to the variance for each response, is to use observation weights, with weights proportional to the inverse variance. 4.7.5.4 Outliers Just as described for Simple Linear Regression 4.7.5.5 High-leverage points Just as described for Simple Linear Regression 4.7.5.6 Collinearity When two (or more) predictor variables are highly correlated, they are said to be collinear. This can reduce the accuracy of the regression coefficients, and can even result in a failure to reject the null hypothesis in significant cases. This can be detected by calculating the variance inflation factor (VIF) for each predictor variable. A value that exceeds 5 or 10 can indicate a problem. When predictors have been identified as collinear, the situation can be remedied by either (a) dropping all but one of the predictors or (b) combining the collinear variables into a single predictor (by taking the average or multiplying them together). "],["chapter-5.html", "Chapter 5 Logistic Regression 5.1 Description 5.2 How it Works 5.3 Evaluating Validity 5.4 Evaluating Fit 5.5 Example Prices of over 50,000 round cut diamonds", " Chapter 5 Logistic Regression 5.1 Description An approach for predicting a the probability that response value \\(Y\\) belongs to a particular category based on one or more predictor values \\(X_1, X_2, ... X_n\\). The probability will always lie between 0 (no chance) and 1 (absolute certainty) and can be given by the following logistic function for the case with a single predictor variable: \\[p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\\] where \\(p(X) = Pr(Y = category|X)\\), which can be read as “the probability that \\(Y\\) is category given \\(X\\). Unlike a linear function, this logistic function will not indicate a probability of an observation belonging to a particular category as negative or greater than 1. Determining whether to treat a particular observation as belonging to a particular category can be made on the basis of the probability returned by this function. It may be reasonable to use a 50% threshold in many cases (\\(p(X) &gt; 0.5\\)), but an analyst may want to be adjust this threshold to meet business needs. You may wish to raise the threshold to reduce false positive classifications or lower it to reduce false negative classifications. 5.2 How it Works A careful observer may not some similarities in the exponents of the formula above and the linear formula discussed in previous chapters. A bit of manipulation yields: \\[\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1X}\\] where the \\(\\frac{p(X)}{1 - p(X)}\\) term is labeled as the odds of the event, such that an odds of \\(1/4\\) yields \\(p(X) = 0.2\\) and an odds of \\(9\\) yields \\(p(X) = 0.9\\). You can confirm this by noting that \\(\\frac{0.9}{1 - 0.9} = 9\\). Taking the logarithm of both sides yields: \\[\\ln{ \\left( \\frac{p(X)}{1 - p(X)} \\right) } = \\beta_0 + \\beta_1X\\] The left-hand side of that equation is called the log odds or logit. Now, our equation looks eerily similar to the linear equation because, in fact, the relationship between “the log odds that the response falls into a certain category given \\(X\\)” and \\(X\\) is linear. That is, for one unit change in \\(X\\), the log odds that the response falls into the indicated category changes by a constant amount \\(\\beta_1\\). This behavior can be extended to the case of multiple predictor variables in a manner analogous to what we have seen for Linear Regression: \\[\\ln{ \\left( \\frac{p(X)}{1 - p(X)} \\right) } = \\beta_0 + \\beta_1x_1 + \\,... \\, + \\beta_px_p\\] This model can also be extended to the case where there are more than two response categories, known as a multinomial logistic regression model, like so (with a single predictor for simplicity): \\[\\ln{ \\left( \\frac{Pr(Y=k|X)}{Pr(Y=K|X)} \\right) } = \\beta_0 + \\beta_1X\\] To do this, given that there are \\(K\\) possible values for \\(Y\\), one possible \\(K\\) is chosen as the default, or baseline value. Consider the example of a model to classify flower species using the iris data set. skimr::skim(iris) Table 5.1: Data summary Name iris Number of rows 150 Number of columns 5 _______________________ Column type frequency: factor 1 numeric 4 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Species 0 1 FALSE 3 set: 50, ver: 50, vir: 50 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Sepal.Length 0 1 5.84 0.83 4.3 5.1 5.80 6.4 7.9 ▆▇▇▅▂ Sepal.Width 0 1 3.06 0.44 2.0 2.8 3.00 3.3 4.4 ▁▆▇▂▁ Petal.Length 0 1 3.76 1.77 1.0 1.6 4.35 5.1 6.9 ▇▁▆▇▂ Petal.Width 0 1 1.20 0.76 0.1 0.3 1.30 1.8 2.5 ▇▁▇▅▃ If you are fitting a logistic regression to the iris data set to predict species, you may set the baseline to be ‘virginica’. The choice of baseline is not important for fitting the model, but it is important for interpreting the estimated \\(\\beta\\) coefficients, as \\(Pr(Y = k|X)\\) is read as the probability that \\(Y\\) is some value other than the baseline \\(k\\) given \\(X\\) and \\(Pr(Y=K|X)\\) is the probability that \\(Y\\) is the baseline value \\(K\\) given \\(X\\). In other works, the left-hand side is the _log odds of \\(k\\) versus \\(K\\) given \\(X\\). This is an interesting point, but not entirely impactful, as inferences or predictions based on this kind of model will be the same. Finally, this expression can be extended to the case of multiple response and predictor variables like so: \\[\\ln{ \\left( \\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)} \\right) } = \\beta_0 + \\beta_1x_1 + \\, ... \\, + \\beta_px_p\\] As an alternative to choosing a baseline category, the softmax coding of a logistic regression model treats all \\(K\\) classes symmetrically, such that the log odds ratio between one categorical value \\(k\\) and another \\(k&#39;\\) can be represented as: \\[ ln{\\left( \\frac{Pr(Y=k|X=x}{Pr(Y=k&#39;|X=x)} \\right) = (\\beta_{k0} - \\beta_{k&#39;0}) + (\\beta_{k1} - \\beta{k&#39;1})x_1 + \\, ... \\, + (\\beta_{kp} - \\beta_{k&#39;p})x_p} \\] Similarly to the linear case, the fit to a logistic model can be fit by an equation. Instead of the least squares method, logistic models are fit by a maximum likelihood method. In the simple case with one predictor and a binary response variable, the _maximum likelihood function_ attempts to estimate \\(\\beta_0\\) and \\(\\beta_1\\) in such a way that the predicted probability \\(\\hat{p}(x_i)\\) matches the observed result as much as possible. This means that, when \\(Y\\) == category \\(\\hat{p}(x_i)\\) should be very close to 1 and when \\(Y\\) != category \\(\\hat{p}(x_i)\\) should be very close to 0. This is accomplished using a likelihood function of the form: \\[\\ell(\\beta_0, \\beta_1) = \\prod_{i:y_i = 1}{p(x_i)} \\prod_{i&#39;:y_{i&#39;}=0}{(1 - p(x_{i&#39;}))}\\] 5.3 Evaluating Validity As can be seen from the underlying math, there are several assumptions inherent in a logistic regression model, regardless of the number of predictor variables or response categories: Since the contribution by each set of predictors in each observation is calculated independently for each response, it is assume that each observation is independent. Each predictor variable is assumed to be independent as well, that is, there is no collinearity between predictors. This can be detected through exploratory visualization using a scatterplot matrix, by calculating variance inflation factors via car::vif, or by other methods. Just as with linear regression models, the goodness-of-fit can be negatively impacted by outliers or high-leverage points. Finally, as has been demonstrated, logistic regression assumes a linear relationship between the log odds of the response belonging to a given category and the predictor variables. This can be a bit more complicated to assess than in the linear case, but a Box-Tidwell test (car::boxTidwell) or scatter plot can help. These methods are demonstrated in the Example. 5.4 Evaluating Fit 5.4.1 Binomial Logistic Regression There are a variety of methods for evaluating the fit of a logistic regression. Unlike a linear regression on a quantitative response, the ultimate output of a classification model (such as a logistic regression) cannot be easily characterized by how close the individual predicted response is to an observed response, in general, because the response either is or is not classified correctly. Instead, population-wide measures such as a confusion matrix5 can be used. Here’s what that looks like for a binomial logistic regression on the iris data set, determining whether a particular flower is of the setosa species. set.seed(6047) # I&#39;m going to start by switching around the classes on a few of the # observations, just to make the confusion matrix more interesting. # Otherwise, our classifier will be _too_ good. skim( iris_data &lt;- iris |&gt; mutate( replace = sample(Species, n()), Species = if_else(runif(n(), 0, 1) &gt; .15, Species, replace), setosa = factor(Species == &quot;setosa&quot;, levels = c(&quot;TRUE&quot;, &quot;FALSE&quot;)), ) |&gt; select(Species, setosa, matches(&quot;(Length|Width)$&quot;)) ) Table 5.2: Data summary Name … &lt;- NULL Number of rows 150 Number of columns 6 _______________________ Column type frequency: factor 2 numeric 4 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Species 0 1 FALSE 3 ver: 52, vir: 50, set: 48 setosa 0 1 FALSE 2 FAL: 102, TRU: 48 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Sepal.Length 0 1 5.84 0.83 4.3 5.1 5.80 6.4 7.9 ▆▇▇▅▂ Sepal.Width 0 1 3.06 0.44 2.0 2.8 3.00 3.3 4.4 ▁▆▇▂▁ Petal.Length 0 1 3.76 1.77 1.0 1.6 4.35 5.1 6.9 ▇▁▆▇▂ Petal.Width 0 1 1.20 0.76 0.1 0.3 1.30 1.8 2.5 ▇▁▇▅▃ # This recipe assumes that `Species` is predicted by all other values, # creates interaction terms, and normalizes all the numeric predictors. (iris_recipe &lt;- recipe(setosa ~ ., data = iris_data) |&gt; step_rm(Species) # no cheating! |&gt; step_interact(setosa ~ Sepal.Length:Sepal.Width) |&gt; step_interact(setosa ~ Petal.Length:Petal.Width) |&gt; step_normalize(all_numeric_predictors())) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 5 ## ## Operations: ## ## Variables removed Species ## Interactions with setosa, Sepal.Length:Sepal.Width ## Interactions with setosa, Petal.Length:Petal.Width ## Centering and scaling for all_numeric_predictors() # Specify the model (iris_model &lt;- logistic_reg() |&gt; set_engine(&quot;glm&quot;) |&gt; set_mode(&quot;classification&quot;)) ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm # Bundle into a workflow (with fit) (iris_workflow &lt;- workflow() |&gt; add_recipe(iris_recipe) |&gt; add_model(iris_model) |&gt; fit(data = iris_data)) ## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: logistic_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────── ## 4 Recipe Steps ## ## • step_rm() ## • step_interact() ## • step_interact() ## • step_normalize() ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────── ## ## Call: stats::glm(formula = ..y ~ ., family = stats::binomial, data = data) ## ## Coefficients: ## (Intercept) Sepal.Length Sepal.Width ## 1.9093 1.8128 -0.5969 ## Petal.Length Petal.Width Sepal.Length_x_Sepal.Width ## 2.4619 7.7695 -0.2847 ## Petal.Length_x_Petal.Width ## -9.3748 ## ## Degrees of Freedom: 149 Total (i.e. Null); 143 Residual ## Null Deviance: 188.1 ## Residual Deviance: 51.93 AIC: 65.93 # Add predictions to the input data iris_predictions &lt;- augment(iris_workflow, iris_data) # Create a confusion matrix (table) (confusion_matrix &lt;- iris_predictions |&gt; count(setosa, .pred_class) |&gt; pivot_wider(names_from = .pred_class, values_from = n)) ## # A tibble: 2 × 3 ## setosa `TRUE` `FALSE` ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 TRUE 45 3 ## 2 FALSE 5 97 In this binary case, the results can be classified in four different ways: A true positive [\\(TP\\)] is a case where the observed value is ‘true’ and the predicted value is ‘true’. A true negative [\\(TN\\)] is a case where the observed value is ‘false’ and the predicted value is ‘false’. A false positive [\\(FP\\)] is a case where the observed value is ‘false’ and the predicted value is ‘true’. A false negative [\\(FN\\)] is a case where the observed value is ‘true’ and the predicted value is ‘false’. The counts of observations in these four ‘buckets’ can be used to calculate a variety of useful measures: Precision [\\(TP/(TP + FP)\\)] is defined as the proportion of predicted positives that are actually positive. Also called positive predictive value. Answers the question: “Of all the flowers the model predicted to be setosa, what fraction actually were?” Recall [\\(TP/(TP + FN)\\)] is defined as the proportion of positive results out of the number of samples which were actually positive. Also called sensitivity. Answers the question: “Of all the flowers that are actually setosa, what fraction did the model identify?” Specificity [\\(TN/(TN + FP)\\)] is defined as the proportion of negative results out of the number of samples which were actually negative. Answers the question: “Of all flowers that were not setosa, what fraction did the model identify?” Accuracy [\\((TP + TN)/(TP + TN + FP + FN)\\)] is the percentage of labels predicted accurately for a sample. Answers the question: “Of all the observations, what fraction were correctly classified?” F Measure is a weighted average of the precision and recall, with best 1 and worst being 0. Cohen’s Kappa is also used to evaluate inter-rater reliability, but if one considers the observed classification to be set by one rater and the predicted classes to be set by another rater, it can be usefully applied to classification models. When applied in this way, \\(\\kappa\\) provides an estimate of how much better the observed accuracy (calculated as shown above) is than the expected accuracy (shown below). For example, if the expected accuracy is 50% (random chance) and the observed accuracy is 95%, \\(\\kappa\\) will be 0.90. This is especially useful when the class distribution is skewed. \\(\\kappa\\) can be calculated as shown: \\[\\begin{align} observations &amp;= n = TP + TN + FP + FN \\\\ accuracy_{obs} &amp;= \\frac{TP + TN}{n}\\\\ accuracy_{exp} &amp;= \\left(\\frac{TP * FP}{obs} + \\frac{TN * FN}{obs}\\right) \\div n\\\\ \\kappa &amp;= \\frac{accuracy_{obs} - accuracy{exp}}{1 - accuracy_{exp}} \\end{align}\\] As described in How it Works, a logistic regression model doesn’t exactly predict the class of each observation, but a set of probabilities that the observation belongs to each class. In the binary case, these are the probability that the observed class is ‘true’ ($p(X)$) and the probability that it is ‘false’ ($1 - p(X)$). By default, if \\(p(X) &gt; 0.5\\), then the predicted class will be ‘true’. This threshold can be manipulated in order to further evaluate the model fit. By plotting the sensitivity against [1 - specificity] for a range of threshold values, you get a received operator characteristic (ROC) chart: (iris_predictions |&gt; roc_curve(truth = setosa, .pred_TRUE) |&gt; autoplot()) The dotted diagonal line represents the probability of randomly guessing the correct class, so you want to be as far from that line as possible! For a theoretical model making perfect predictions, the curve would rise straight up the left side then across the top. The area under the curve (AUC) is a value between 0 and 1 that provides a quantitative measurement of the performance indicated by the ROC curve. The closer this value is to 1, the better the model has performed. # Define a set of metrics using the `yardstick` package eval_metrics &lt;- metric_set(ppv, recall, specificity, accuracy, f_meas, kap, roc_auc) eval_metrics( data = iris_predictions, truth = setosa, estimate = .pred_class, .pred_TRUE # to be passed to `roc_auc()` ) ## # A tibble: 7 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 ppv binary 0.9 ## 2 recall binary 0.938 ## 3 specificity binary 0.951 ## 4 accuracy binary 0.947 ## 5 f_meas binary 0.918 ## 6 kap binary 0.879 ## 7 roc_auc binary 0.978 5.4.2 Multinomial Logistic Regression When expanding our predictive value to predicting all classes of iris species: # This recipe assumes that `Species` is predicted by all other values, # creates interaction terms, and normalizes all the numeric predictors. (iris_recipe &lt;- recipe(Species ~ ., data = iris_data) |&gt; step_rm(setosa) # No cheating! |&gt; step_interact(Species ~ Sepal.Length:Sepal.Width) |&gt; step_interact(Species ~ Petal.Length:Petal.Width) |&gt; step_normalize(all_numeric_predictors())) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 5 ## ## Operations: ## ## Variables removed setosa ## Interactions with Species, Sepal.Length:Sepal.Width ## Interactions with Species, Petal.Length:Petal.Width ## Centering and scaling for all_numeric_predictors() # Specify the model (iris_model &lt;- multinom_reg() # Since there are three possible classes |&gt; set_engine(&quot;nnet&quot;) # Default for `multinom_reg` |&gt; set_mode(&quot;classification&quot;)) ## Multinomial Regression Model Specification (classification) ## ## Computational engine: nnet # Bundle into a workflow (with fit) (iris_workflow &lt;- workflow() |&gt; add_recipe(iris_recipe) |&gt; add_model(iris_model) |&gt; fit(data = iris_data)) ## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: multinom_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────── ## 4 Recipe Steps ## ## • step_rm() ## • step_interact() ## • step_interact() ## • step_normalize() ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────── ## Call: ## nnet::multinom(formula = ..y ~ ., data = data, trace = FALSE) ## ## Coefficients: ## (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width Sepal.Length_x_Sepal.Width ## versicolor 0.3185672 -1.562520 -4.3193520 5.836776 14.121343 5.036399 ## virginica 0.9500096 1.757604 -0.1347156 2.114512 4.658676 -1.056959 ## Petal.Length_x_Petal.Width ## versicolor -20.916383 ## virginica -5.287086 ## ## Residual Deviance: 111.6001 ## AIC: 139.6001 # Add predictions to the input data iris_predictions &lt;- augment(iris_workflow, iris_data) # Create a confusion matrix (table) (confusion_matrix &lt;- iris_predictions |&gt; count(Species, .pred_class) |&gt; pivot_wider(names_from = .pred_class, values_from = n)) ## # A tibble: 3 × 4 ## Species setosa versicolor virginica ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 setosa 43 3 2 ## 2 versicolor 3 45 4 ## 3 virginica 2 4 44 Here we see that our relatively “un-tuned” model does a good job of identifying iris species with just a few mis-classifications (due in large part to our ‘tweaks’ to the data set). Each row represents the true class of the flower while each column represents the predicted class of each flower. In a perfect world, we would only have numbers on the diagonal. We can examine the same metrics as we did in the binomial case: eval_metrics &lt;- metric_set(ppv, recall, specificity, accuracy, f_meas, kap, roc_auc) eval_metrics( data = iris_predictions, truth = Species, estimate = .pred_class, .pred_setosa, # to be passed to `roc_auc()` .pred_versicolor, # / / .pred_virginica # / ) ## # A tibble: 7 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 ppv macro 0.880 ## 2 recall macro 0.880 ## 3 specificity macro 0.940 ## 4 accuracy multiclass 0.88 ## 5 f_meas macro 0.880 ## 6 kap multiclass 0.820 ## 7 roc_auc hand_till 0.956 5.5 Example 5.5.1 Dataset For our example logistic regression, let’s attempt to classify diamonds by expected ‘price class’ using the diamonds dataset. R: Prices of over 50,000 round cut diamonds diamondsR Documentation Prices of over 50,000 round cut diamonds Description A dataset containing the prices and other attributes of almost 54,000 diamonds. The variables are as follows: Usage diamonds Format A data frame with 53940 rows and 10 variables: priceprice in US dollars (\\$326&ndash;\\$18,823) caratweight of the diamond (0.2&ndash;5.01) cutquality of the cut (Fair, Good, Very Good, Premium, Ideal) colordiamond colour, from D (best) to J (worst) claritya measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)) xlength in mm (0&ndash;10.74) ywidth in mm (0&ndash;58.9) zdepth in mm (0&ndash;31.8) depthtotal depth percentage = z / mean(x, y) = 2 * z / (x + y) (43&ndash;79) tablewidth of top of diamond relative to widest point (43&ndash;95) skim(diamonds) Table 5.3: Data summary Name diamonds Number of rows 53940 Number of columns 10 _______________________ Column type frequency: factor 3 numeric 7 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts cut 0 1 TRUE 5 Ide: 21551, Pre: 13791, Ver: 12082, Goo: 4906 color 0 1 TRUE 7 G: 11292, E: 9797, F: 9542, H: 8304 clarity 0 1 TRUE 8 SI1: 13065, VS2: 12258, SI2: 9194, VS1: 8171 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist carat 0 1 0.80 0.47 0.2 0.40 0.70 1.04 5.01 ▇▂▁▁▁ depth 0 1 61.75 1.43 43.0 61.00 61.80 62.50 79.00 ▁▁▇▁▁ table 0 1 57.46 2.23 43.0 56.00 57.00 59.00 95.00 ▁▇▁▁▁ price 0 1 3932.80 3989.44 326.0 950.00 2401.00 5324.25 18823.00 ▇▂▁▁▁ x 0 1 5.73 1.12 0.0 4.71 5.70 6.54 10.74 ▁▁▇▃▁ y 0 1 5.73 1.14 0.0 4.72 5.71 6.54 58.90 ▇▁▁▁▁ z 0 1 3.54 0.71 0.0 2.91 3.53 4.04 31.80 ▇▁▁▁▁ 5.5.2 Response Categories Because I know I want to predict a ‘price class’ as a category instead of as continuous value, I should try to determine how large the range of prices in each category should be. Ideally, each price category should contain a comparable number of observations. You’d also want to consider the business use case here, but since this is an example of logistic regression modeling more than a primer on the diamond trade, let’s stick to making roughly equal size groups. Let’s take a look a the distribution of price. ggplot(diamonds, aes(price)) + geom_histogram(bins = 50) + theme_minimal() Oh, well, that’s troublesome. Looks like we’ll need to log-transform price if we want it to be evenly distributed across the range. ggplot(diamonds, aes(log(price))) + geom_histogram(bins = 50) + theme_minimal() That’s much better. Now, let’s identify the optimal interval size: (tibble(buckets = seq_len(10)) |&gt; mutate(.calc = map(buckets, ~ ( diamonds |&gt; mutate(bucket = cut_interval(log(price), .x)) |&gt; count(bucket) |&gt; summarise( mean_obs = mean(n), sd_obs = sd(n), min_obs = min(n), max_obs = max(n), range = max_obs - min_obs, ) ))) |&gt; unnest(.calc)) ## # A tibble: 10 × 6 ## buckets mean_obs sd_obs min_obs max_obs range ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 53940 NA 53940 53940 0 ## 2 2 26970 617. 26534 27406 872 ## 3 3 17980 2635. 15241 20498 5257 ## 4 4 13485 3230. 9581 16953 7372 ## 5 5 10788 2991. 7144 13443 6299 ## 6 6 8990 3078. 5200 13001 7801 ## 7 7 7706. 2854. 3665 11568 7903 ## 8 8 6742. 2491. 2618 9681 7063 ## 9 9 5993. 2195. 1997 8258 6261 ## 10 10 5394 2106. 1553 8400 6847 It looks like 5 groups provides a good balance between the class sizes and the number of classes. 5.5.3 Correlations Let’s start by establishing the correlations amongst the numeric variables in this dataset: # Use a scatterplot matrix to identify correlations columns &lt;- c(&quot;price&quot;, &quot;carat&quot;, &quot;depth&quot;, &quot;table&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;) GGally::ggpairs(diamonds, columns = columns, progress = F) We can make a few observations here: The various size parameters: carat, x, y, and z are all highly correlated, as may be expected. The shape of the distribution for carat indicates that it may be best used when log-transformed (a left-biased distribution with a long tail). price is very highly correlated with carat, as might be expected from a layman’s understanding of how diamonds are priced. Bigger is better! There are a few categorical variables as well that we should examine: ggplot(diamonds, aes(price, carat, color = cut)) + geom_point(alpha = 0.5) + theme_minimal() ggplot(diamonds, aes(price, carat, color = color)) + geom_point(alpha = 0.5) + theme_minimal() ggplot(diamonds, aes(price, carat, color = clarity)) + geom_point(alpha = 0.5) + theme_minimal() There appears to be some variation in price/carat based on these categorical values, but it’s not extremely pronounced. The patterns are perhaps more easily observable on the correlogram. columns &lt;- c(&quot;price&quot;, &quot;cut&quot;, &quot;color&quot;, &quot;clarity&quot;) GGally::ggpairs(diamonds, columns = columns, progress = F) There appears to be some correlation between color, cut, and clarity, with the most pronounced between color and cut. If we knew more about the diamond pricing market, we might infer that certain cuts are preferred for certain colors of diamond, but that’s just conjecture at this point. We’ll choose to add an interaction term for color:cut, but there’s probably room for more experimentation on this point. 5.5.4 High Leverage Points Recall, that high-leverage points are those observations where the observed values fall well outside the range of the majority of the observed values. Since we only have one continuous predictor variable (carat), any high-leverage points should be pretty easy to find. (Note, recall that above we indicated a that we should log-transform carat due to its distribution, so we should do that here as well). ggplot(diamonds, aes(factor(1), log(carat))) + geom_boxplot() + theme_minimal() Looks like there are two values for carat that fall way outside the range of the majority of the values. Let’s find them: (diamonds |&gt; arrange(desc(carat)) |&gt; head(2) |&gt; pull(carat) |&gt; log() |&gt; round(2)) ## [1] 1.61 1.50 Just for fun, let’s check the normal range for carat: mean_carat &lt;- mean(log(diamonds$carat)) |&gt; round(2) sd_carat &lt;- sd(log(diamonds$carat)) |&gt; round(2) paste(mean_carat, &quot;+/-&quot;, sd_carat) ## [1] &quot;-0.39 +/- 0.58&quot; Those values are more than 3 standard deviations away from the mean of log carat. To be on the safe side, let’s exclude any observations where log(carat) lies 3 or more standard deviations from the mean. 5.5.5 Fit and Check From exploring the dataset, I’ve decided to: Log-transform price then add each transformed price to one of 5 classes, depending on its value. Use carat, cut, clarity, and color as predictor variables. Log-transform carat prior to training the model. Remove observations where log(carat) is 3 or more standard deviations away from the mean as high-leverage observations. Add an interaction parameter for color:cut. set.seed(800633) # Modification to response variables should not be part of the workflow mod_diamonds &lt;- mutate(diamonds, price_cat = cut_interval(log(price), 5)) mod_diamonds_split &lt;- initial_split(mod_diamonds) # Specify the recipe, using the preparation steps described above (log_reg_recipe &lt;- recipe(price_cat ~ carat + cut + color + clarity, data = mod_diamonds) |&gt; step_log(carat) |&gt; step_filter(abs(carat - mean(carat)) &lt; (sd(carat) * 3)) |&gt; step_dummy(all_nominal_predictors()) |&gt; step_interact(price_cat ~ starts_with(&quot;color_&quot;):starts_with(&quot;cut_&quot;))) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Operations: ## ## Log transformation on carat ## Row filtering using abs(carat - mean(carat)) &lt; (sd(carat) * 3) ## Dummy variables from all_nominal_predictors() ## Interactions with price_cat, starts_with(&quot;color_&quot;):starts_with(&quot;cut_&quot;) # Setup the model (log_reg_model &lt;- multinom_reg() |&gt; set_engine(&quot;nnet&quot;) |&gt; set_mode(&quot;classification&quot;)) ## Multinomial Regression Model Specification (classification) ## ## Computational engine: nnet # Bundle the recipe and model into a workflow, fit the model (log_reg_workflow &lt;- workflow() |&gt; add_recipe(log_reg_recipe) |&gt; add_model(log_reg_model) |&gt; fit(data = training(mod_diamonds_split))) ## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: multinom_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────── ## 4 Recipe Steps ## ## • step_log() ## • step_filter() ## • step_dummy() ## • step_interact() ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────── ## Call: ## nnet::multinom(formula = ..y ~ ., data = data, trace = FALSE) ## ## Coefficients: ## (Intercept) carat cut_1 cut_2 cut_3 cut_4 color_1 color_2 ## (6.6,7.41] 14.77133 14.98926 4.106441 -3.569287 1.3668315 -2.011030 -2.848696 -0.03877709 ## (7.41,8.22] 25.23687 32.12698 3.895234 -3.037634 0.5433772 -1.746210 -5.919968 0.17077340 ## (8.22,9.03] 27.02919 46.56542 5.066777 -3.195376 0.9064511 -1.378375 -8.423160 -0.10510982 ## (9.03,9.84] 21.85345 59.44266 7.601931 -4.935027 2.6772676 -1.714478 -9.793854 -1.65460281 ## color_3 color_4 color_5 color_6 clarity_1 clarity_2 clarity_3 clarity_4 ## (6.6,7.41] 2.1058502 0.06402263 -0.39966774 -0.6704520 7.806147 0.7449121 -0.2797165 0.6140537 ## (7.41,8.22] 0.8627596 -0.49520457 0.08630286 -0.1890811 15.557550 0.3599709 -0.3223965 2.0028156 ## (8.22,9.03] 0.6231886 -0.06351782 0.14218674 -0.8032102 21.285218 -1.0968904 1.0169551 0.8260622 ## (9.03,9.84] 0.4438435 -0.44292761 0.43035152 -1.9180478 29.222613 -5.9869350 3.7545372 -1.1978647 ## clarity_5 clarity_6 clarity_7 color_1_x_cut_1 color_1_x_cut_2 color_1_x_cut_3 ## (6.6,7.41] -0.4483736 0.3632309 0.6256839 -1.888081 -5.772038 1.358870 ## (7.41,8.22] -0.5171474 0.5503920 0.8737247 -6.663059 -3.452039 -2.487619 ## (8.22,9.03] -1.2763157 0.0479671 0.6268565 -6.465506 -2.468129 -2.750882 ## (9.03,9.84] 0.6341891 0.9096949 1.3602075 -12.137445 2.230262 -5.166862 ## color_1_x_cut_4 color_2_x_cut_1 color_2_x_cut_2 color_2_x_cut_3 color_2_x_cut_4 ## (6.6,7.41] -2.0890335 1.5252801 -3.0902028 0.8809812 0.3365358 ## (7.41,8.22] 1.4108126 -0.9267690 -1.3385217 -2.0378257 1.5823993 ## (8.22,9.03] -0.5081945 -0.7446158 0.7016114 -3.0166846 3.4134568 ## (9.03,9.84] 0.2651442 1.2777073 0.2345068 -1.0267471 3.2055242 ## color_3_x_cut_1 color_3_x_cut_2 color_3_x_cut_3 color_3_x_cut_4 color_4_x_cut_1 ## (6.6,7.41] -0.7423168 -2.467593 0.3167168 0.4632866 1.991268 ## (7.41,8.22] 0.6897983 -2.892903 0.5723597 0.5996124 2.938301 ## (8.22,9.03] 2.5698649 -2.275121 1.6777016 1.3828688 2.764084 ## (9.03,9.84] 2.3537641 -2.189823 2.9626609 1.7180650 3.093712 ## color_4_x_cut_2 color_4_x_cut_3 color_4_x_cut_4 color_5_x_cut_1 color_5_x_cut_2 ## (6.6,7.41] -4.183403 1.27063921 0.3216012 1.0628629 0.03580568 ## (7.41,8.22] -4.353914 3.34454302 0.5546513 -1.2086507 0.78604881 ## (8.22,9.03] -5.145836 3.72353328 1.4893282 -0.1674063 0.21505487 ## (9.03,9.84] -2.133068 0.02659431 -0.1138213 -2.9317166 2.95632557 ## color_5_x_cut_3 color_5_x_cut_4 color_6_x_cut_1 color_6_x_cut_2 color_6_x_cut_3 ## (6.6,7.41] -1.2692962 -0.5879730 -0.3521551 0.02457326 -1.1774001 ## (7.41,8.22] 1.7574518 -0.4392463 -4.4523729 2.43067566 0.3927754 ## (8.22,9.03] 2.1639698 0.2834687 -2.1473629 1.11493030 0.1150996 ## (9.03,9.84] 0.8466614 0.4828583 1.4863675 -2.46947278 3.4413292 ## color_6_x_cut_4 ## (6.6,7.41] 0.1110854 ## (7.41,8.22] -0.3388539 ## (8.22,9.03] 0.6050693 ## (9.03,9.84] -0.7874513 ## ## Residual Deviance: 30240.06 ## AIC: 30584.06 # Add predictions to training data log_reg_predictions &lt;- augment(log_reg_workflow, testing(mod_diamonds_split)) # Check performance using the same set of metrics as we did previously eval_metrics &lt;- metric_set( ppv, recall, specificity, accuracy, f_meas, kap, roc_auc ) eval_metrics( data = log_reg_predictions, truth = price_cat, estimate = .pred_class, `.pred_[5.79,6.6]`, `.pred_(6.6,7.41]`, `.pred_(7.41,8.22]`, `.pred_(8.22,9.03]`, `.pred_(9.03,9.84]` ) ## # A tibble: 7 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 ppv macro 0.851 ## 2 recall macro 0.842 ## 3 specificity macro 0.961 ## 4 accuracy multiclass 0.847 ## 5 f_meas macro 0.846 ## 6 kap multiclass 0.806 ## 7 roc_auc hand_till 0.978 The metrics look pretty good, let’s take a look at the confusion matrix… (log_reg_predictions |&gt; count(price_cat, .pred_class) |&gt; pivot_wider(names_from = .pred_class, values_from = n)) ## # A tibble: 5 × 6 ## price_cat `[5.79,6.6]` `(6.6,7.41]` `(7.41,8.22]` `(8.22,9.03]` `(9.03,9.84]` ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 [5.79,6.6] 1546 454 NA NA NA ## 2 (6.6,7.41] 290 2636 292 NA NA ## 3 (7.41,8.22] 1 130 2680 233 NA ## 4 (8.22,9.03] NA NA 202 2984 191 ## 5 (9.03,9.84] NA NA NA 267 1579 All told, our classifier seems to be working OK. Most of the time, we’re correctly predicted the price category for each observation in the testing set, and when we mis-classify the price category we are (with one single exception) picking the price category above or below the true category. Definition from Wikipedia.↩︎ "],["references.html", "Chapter 6 References", " Chapter 6 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
