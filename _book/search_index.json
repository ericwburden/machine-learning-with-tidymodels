[["index.html", "Machine Learning with TidyModels Chapter 1 About 1.1 Software Used", " Machine Learning with TidyModels Eric Burden 2022-10-14 Chapter 1 About Machine Learning with TidyModels is the result of my desire to gain a foundational understanding of machine learning and machine learning techniques without overcomplicating the process by wrestling with the huge variety of models and modeling packages available in the R ecosystem. The promise of the tidymodels meta-package is a unified interface for preparing data, training models, and leveraging the results. I find this to be an eminently sensible approach and see in tidymodels an opportunity to let the software “get out of the way” and let me focus on the ML algorithms themselves. For me, this book serves as a training exercise. I hope that, when finished, it will serve as a valuable reference. 1.1 Software Used This book was developed with version 4.2.1 of the R language and version 1.0.0 of the tidymodels meta-package. "],["the-tidymodels-workflow.html", "Chapter 2 The Tidymodels Workflow 2.1 About Tidymodels 2.2 Philosophy in Practice 2.3 Workflow Steps", " Chapter 2 The Tidymodels Workflow 2.1 About Tidymodels tidymodels is an R meta-package, meaning it is essentially a bundle of R packages organized around a common theme. In this case, the theme is “modeling and machine learning using tidyverse principles”. In order to make sense of that, one needs to have a general understanding of what the tidyverse is and its philosophy. The tidyverse is yet another R meta-package organized around the theme of “data science”. The tidyverse packages share a common design philosophy, grammar, and many underlying data structures, such that working with tidyverse packages feels to the user as if they are leveraging a single, wide-ranging library by a single author (in the best case). There is an entire book on the subject of using the tidyverse to perform common data science tasks(Wickham and Grolemund 2017), but for the purposes of this book, it is enough to understand the four principles upon which the tidyverse is built, that is(Wickham 2022): Re-use existing data structures Compose simple functions with the pipe Embrace functional programming Design for humans 2.1.1 Re-use existing data structures Where possible, tidyverse and, by extension, tidymodels packages rely on common data structures, typically a tibble or data.frame (a tibble is essentially an improved data.frame with much of the same API). For more focused operations on single data types, this is more often a standard R vector or an S3 object that shares the vector API. In short, data structures produced and consumed by tidyverse and tidymodels packages should behave as expected by someone who is familiar with base-R data structures. Data frames should be “tidy”, consisting of variables in columns and observations in rows(Wickham 2014). 2.1.2 Compose simple functions with pipes The magrittr library(Bache and Wickham 2022) has long provided a “pipe” (%&gt;%) operator in R. More recently, a pipe operator (|&gt;) has been introduced to the language itself. There are some subtle differences in usage between the two, but that is beyond the scope of this work. In an effort to be as forward-looking as possible, examples in this book will use the native pipe operator. Regardless of which operator is chosen, however, the effect is similar to the pipe operator (|) that many will be familiar with from the Unix shell, in that the pipe serves as an infix operator that takes the result of calling the left-hand side and passes that result as the first argument to the operation (usually a function call) on the right-hand side. This makes the examples below equivalent: output &lt;- paste(unlist(strsplit(&quot;Replace:colons:with:spaces&quot;, &quot;:&quot;)), collapse = &quot; &quot;) input_str &lt;- &quot;Replace:colons:with:spaces&quot; split_input &lt;- strsplit(input_str, &quot;:&quot;) split_input_vec &lt;- unlist(split_input) output &lt;- paste(split_input_vec, collapse = &quot; &quot;) output &lt;- &quot;Replace:colons:with:spaces&quot; |&gt; strsplit(&quot;:&quot;) |&gt; unlist() |&gt; paste(collapse = &quot; &quot;) output &lt;- &quot;Replace:colons:with:spaces&quot; |&gt; strsplit(&quot;:&quot;) |&gt; unlist() |&gt; paste(collapse = &quot; &quot;) (output &lt;- &quot;Replace:colons:with:spaces&quot; |&gt; strsplit(&quot;:&quot;) |&gt; unlist() |&gt; paste(collapse = &quot; &quot;)) ## [1] &quot;Replace colons with spaces&quot; The first and most obvious benefit of this approach is readability. Using the pipe syntax, operations can be written in the order they are performed, as opposed to the “inside-out” approach of the first example. We can also avoid cluttering up the R environment with as in the “intermediate variables” approach of the second example. Less obvious are the benefits to coding habits; effective use of the pipe operator encourages sensible function naming and organization. The trade-off is that the programmer needs to be careful not to take multiple piped operations too far, breaking up long series of pipes with some intermediate variables as appropriate. 2.1.3 Embrace functional programming There’s a lot to say about what functional programming actually is, but in this context it mostly means: immutable objects and copy-on-modify semantics, the frequent use of “pure” functions that do not have side-effects, using generic functions where possible and appropriate, and abstracting over loops in a more “iterator-like” fashion. 2.1.4 Design for humans This is a bit subjective, but the tidyverse and tidymodels strive to provide APIs that are intuitive for human users and friendly to the IDE such that autocomplete is a helpful tool for discovering functionality. Generally, being autocomplete-friendly means grouping function names under common prefixes like [str_subset, str_detect, str_extract]and [add_model, add_recipe, add_formula] as opposed to alternatives like [subset_str, detect_str, extract_str] and [model_add, recipe_add, forumal_add]or the like. 2.2 Philosophy in Practice These pieces of tidyverse philosophy adopted by tidymodels yield a common way of working with and thinking about code written using the tidymodels collection of packages. The most prominent example of this is the tidymodels workflow, or, rather, the workflow package which is bundled into tidymodels. Throughout this book, the term workflow object will be used to refer to an R object to which various pre-processing, modeling, and post-processing steps can be added to facilitate repeatable and ergonomic ML code. Commonly, this will consist of splitting the data (using the rsample package), creating and adding a recipe (from the recipes package), creating and adding a model (from the parsnip package), fitting the model, then using the model to make inferences or predictions. The recipe and model steps may be fairly dense as well, but every step along the way should ideally reflect the principles described above. This approach can be better demonstrated than described, as show below: # Split the data into testing and training steps mtcars_split &lt;- initial_split(mtcars) # Pre-process the data using a recipe (example_recipe &lt;- recipe(mpg ~ cyl + disp + hp + wt, data = training(mtcars_split)) |&gt; step_mutate_at(cyl, fn = factor) |&gt; step_log(disp, hp) |&gt; step_interact(terms = ~ disp:hp)) ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Operations: ## ## Variable mutation for cyl ## Log transformation on disp, hp ## Interactions with disp:hp # Setup up the model (example_model &lt;- linear_reg() |&gt; set_engine(&quot;glm&quot;) |&gt; set_mode(&quot;regression&quot;)) ## Linear Regression Model Specification (regression) ## ## Computational engine: glm # Bundle the recipe and model into a workflow, fit the model (example_workflow &lt;- workflow() |&gt; add_recipe(example_recipe) |&gt; add_model(example_model) |&gt; fit(data = training(mtcars_split))) ## ══ Workflow [trained] ══════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────────────────────────── ## 3 Recipe Steps ## ## • step_mutate_at() ## • step_log() ## • step_interact() ## ## ── Model ─────────────────────────────────────────────────────────────────────────────────────────── ## ## Call: stats::glm(formula = ..y ~ ., family = stats::gaussian, data = data) ## ## Coefficients: ## (Intercept) cyl6 cyl8 disp hp wt disp_x_hp ## 200.0310 -0.7617 -1.8367 -31.3085 -32.7479 -2.2093 5.9062 ## ## Degrees of Freedom: 23 Total (i.e. Null); 17 Residual ## Null Deviance: 747.4 ## Residual Deviance: 92.33 AIC: 116.4 # Add predictions to training data example_predictions &lt;- augment(example_workflow, testing(mtcars_split)) # Check performance metrics(example_predictions, mpg, .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 2.44 ## 2 rsq standard 0.786 ## 3 mae standard 2.31 To the untrained eye, it may appear that all the functions used in this code are from a common library, given the similarity in their usage and the ease with which the outputs of one operation are passed along to the next, but there are actually at least five different R libraries at play here! This is a key benefit of tidymodels, the bundled packages work seamlessly together to provide a consistent developer experience. 2.3 Workflow Steps The typical steps involved in using tidymodels to train and work with ML models are described below. These steps may be undertaken in a different order than listed here and individual steps may not be as cleanly separated as indicated. There may be additional steps for training and testing multiple models or other complex tasks, but the following is a good, general guide. 2.3.1 Data Splitting In the first step, you’ll primary use the rsample (Silge et al. 2022) package to divide up the data for training, validating, and testing the model. At a minimum, this generally involves splitting the data set into training and testing samples. Models, sometimes many different models, can be trained on the training sample, but the testing sample should be used only once, to validate and measure model performance on “real” data at the end. You can also set up your data for re-sampling in this step. 2.3.2 Data Preparation The next step consists of creating a specification for data pre-processing using the recipes (Kuhn and Wickham 2022) package. This can include a number of steps, including specifying the relationships between the response and predictor variables, data cleaning, transforming predictors, etc. 2.3.3 Model Specification In this step, the parsnip (Kuhn and Vaughan 2022) package is typically used to create a model specification. This includes indicating the type of the model, the model engine, the model mode, and providing relevant parameters. 2.3.4 Bundling a Workflow This step utilizes the workflows (Vaughan and Couch 2022) package to bundle together the recipe and model specifications into a single workflow object, which can then be used to fit the model, make predictions, or other operations. 2.3.5 Checking Results In this step, the yardstick (Kuhn, Vaughan, and Hvitfeldt 2022) package is used to measure model performance. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
