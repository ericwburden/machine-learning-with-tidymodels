---
editor_options: 
  markdown: 
    wrap: 88
bibliography: references.bib
---

# Logistic Regression {#chapter-5}

## Description {#ch5-description}

An approach for predicting a the probability that response value $Y$ belongs to a
particular *category* based on one or more predictor values $X_1, X_2, ... X_n$. The
probability will always lie between 0 (no chance) and 1 (absolute certainty) and can be
given by the following *logistic function* for the case with a single predictor
variable:

$$p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}$$ where
$p(X) = Pr(Y = category|X)$, which can be read as "the probability that $Y$ is
`category` given $X$.

Unlike a linear function, this *logistic* function will not indicate a probability of an
observation belonging to a particular category as negative or greater than 1.
Determining whether to treat a particular observation as belonging to a particular
category can be made on the basis of the probability returned by this function. It may
be reasonable to use a 50% threshold in many cases ($p(X) > 0.5$), but an analyst may
want to be adjust this threshold to meet business needs. You may wish to raise the
threshold to reduce false positive classifications or lower it to reduce false negative
classifications.

## How it Works {#ch5-how-it-works}

A careful observer may not some similarities in the exponents of the formula above and
the linear formula discussed in previous chapters. A bit of manipulation yields:

$$\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}$$

where the $\frac{p(X)}{1 - p(X)}$ term is labeled as the *odds* of the event, such that
an *odds* of $1/4$ yields $p(X) = 0.2$ and an *odds* of $9$ yields $p(X) = 0.9$. You can
confirm this by noting that $\frac{0.9}{1 - 0.9} = 9$. Taking the logarithm of both
sides yields:

$$\ln{ \left( \frac{p(X)}{1 - p(X)} \right) } = \beta_0 + \beta_1X$$

The left-hand side of that equation is called the *log odds* or *logit*. Now, our
equation looks *eerily* similar to the linear equation because, in fact, the
relationship between "the log odds that the response falls into a certain category given
$X$" and $X$ is linear. That is, for one unit change in $X$, the log odds that the
response falls into the indicated category changes by a constant amount $\beta_1$. This
behavior can be extended to the case of multiple predictor variables in a manner
analogous to what we have seen for [Linear Regression](#chapter-4):

$$\ln{ \left( \frac{p(X)}{1 - p(X)} \right) } = \beta_0 + \beta_1x_1 + \,... \, + \beta_px_p$$

This model can also be extended to the case where there are more than two response
categories, known as a *multinomial logistic regression* model, like so (with a single
predictor for simplicity):

$$\ln{ \left( \frac{Pr(Y=k|X)}{Pr(Y=K|X)} \right) } = \beta_0 + \beta_1X$$ To do this,
given that there are $K$ possible values for $Y$, one possible $K$ is chosen as the
default, or *baseline* value. Consider the example of a model to classify flower species
using the `iris` data set.

```{r echo=FALSE}
skimr::skim(iris)
```

If you are fitting a logistic regression to the `iris` data set to predict species, you
may set the *baseline* to be 'virginica'. The choice of baseline is not important for
fitting the model, but it is important for interpreting the estimated $\beta$
coefficients, as $Pr(Y = k|X)$ is read as the probability that $Y$ is some value other
than the baseline $k$ given $X$ and $Pr(Y=K|X)$ is the probability that $Y$ is the
baseline value $K$ given $X$. In other works, the left-hand side is the \_log odds of
$k$ versus $K$ given $X$. This is an interesting point, but not entirely impactful, as
inferences or predictions based on this kind of model will be the same. Finally, this
expression can be extended to the case of multiple response and predictor variables like
so:

$$\ln{ \left( \frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)} \right) } = \beta_0 + \beta_1x_1 + \, ... \, + \beta_px_p$$

As an alternative to choosing a baseline category, the *softmax* coding of a logistic
regression model treats all $K$ classes symmetrically, such that the log odds ratio
between one categorical value $k$ and another $k'$ can be represented as:

$$
ln{\left( \frac{Pr(Y=k|X=x}{Pr(Y=k'|X=x)} \right) = (\beta_{k0} - \beta_{k'0}) + (\beta_{k1} - \beta{k'1})x_1 + \, ... \, + (\beta_{kp} - \beta_{k'p})x_p}
$$

Similarly to the linear case, the fit to a logistic model can be fit by an equation.
Instead of the least squares method, logistic models are fit by a *maximum likelihood*
method. In the simple case with one predictor and a binary response variable, the
\_maximum likelihood function\_ attempts to estimate $\beta_0$ and $\beta_1$ in such a
way that the predicted probability $\hat{p}(x_i)$ matches the observed result as much as
possible. This means that, when $Y$ == `category` $\hat{p}(x_i)$ should be very close to
1 and when $Y$ != `category` $\hat{p}(x_i)$ should be very close to 0. This is
accomplished using a *likelihood function* of the form:

$$\ell(\beta_0, \beta_1) = \prod_{i:y_i = 1}{p(x_i)} \prod_{i':y_{i'}=0}{(1 - p(x_{i'}))}$$

## Evaluating Validity {#ch5-evaluating-validity}

As can be seen from the underlying math, there are several assumptions inherent in a
logistic regression model, regardless of the number of predictor variables or response
categories:

-   Since the contribution by each set of predictors in each observation is calculated
    independently for each response, it is assume that *each observation is
    independent*.

-   Each predictor variable is assumed to be independent as well, that is, there is no
    [collinearity](#ch4-collinearity) between predictors. This can be detected through
    exploratory visualization using a scatterplot matrix or by calculating *variance
    inflation factors* via `car::vif` or other methods.

-   Just as with linear regression models, the goodness-of-fit can be negatively
    impacted by [outliers](#ch3-outliers) or [high-leverage
    points](#ch3-high-leverage-points).

-   Finally, as has been demonstrated, logistic regression assumes a linear relationship
    between the log odds of the response belonging to a given category and the predictor
    variables. This can be a bit more complicated to assess than in the linear case, but
    a Box-Tidwell test (`car::boxTidwell`) or scatter plot can help. These methods are
    demonstrated in the [Example](ch5-example).

## Evaluating Fit {#ch5-evaluating-fit}

### Binomial Logistic Regression {#ch5-binomial-logistic-regression}

There are a variety of methods for evaluating the fit of a logistic regression. Unlike a
linear regression on a quantitative response, the ultimate output of a classification
model (such as a logistic regression) cannot be easily characterized by how *close* the
individual predicted response is to an observed response, in general, because the
response either *is* or *is not* classified correctly. Instead, population-wide measures
such as a *confusion matrix*[^1] can be used. Here's what that looks like for a binomial
logistic regression on the `iris` data set, determining whether a particular flower is
of the *setosa* species.

[^1]: Definition from [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix).

```{r}
set.seed(6047)

# I'm going to start by switching around the classes on a few of the
# observations, just to make the confusion matrix more interesting. 
# Otherwise, our classifier will be _too_ good.
(iris_data
  <- iris
  |> mutate(
    replace = sample(Species, n()),
    Species = if_else(runif(n(), 0, 1) > .15, Species, replace),
    setosa  = factor(Species == "setosa", levels = c("TRUE", "FALSE")),
  )
  |> select(Species, setosa, matches("(Length|Width)$")))

# This recipe assumes that `Species` is predicted by all other values,
# creates interaction terms, and normalizes all the numeric predictors.
(iris_recipe
  <- recipe(setosa ~ ., data = iris_data)
  |> step_rm(Species) # no cheating!
  |> step_interact(setosa ~ Sepal.Length:Sepal.Width)
  |> step_interact(setosa ~ Petal.Length:Petal.Width)
  |> step_normalize(all_numeric_predictors()))

# Specify the model
(iris_model
  <- logistic_reg()
  |> set_engine("glm")
  |> set_mode("classification"))

# Bundle into a workflow (with fit)
(iris_workflow
  <- workflow()
  |> add_recipe(iris_recipe)
  |> add_model(iris_model)
  |> fit(data = iris_data))

# Add predictions to the input data
iris_predictions <- augment(iris_workflow, iris_data)

# Create a confusion matrix (table)
(confusion_matrix
  <- iris_predictions
  |> count(setosa, .pred_class)
  |> pivot_wider(names_from = .pred_class, values_from = n))
```

In this binary case, the results can be classified in four different ways:

-   A *true positive* [$TP$] is a case where the observed value is 'true' and the
    predicted value is 'true'.

-   A *true negative* [$TN$] is a case where the observed value is 'false' and the
    predicted value is 'false'.

-   A *false positive* [$FP$] is a case where the observed value is 'false' and the
    predicted value is 'true'.

-   A *false negative* [$FN$] is a case where the observed value is 'true' and the
    predicted value is 'false'.

The counts of observations in these four 'buckets' can be used to calculate a variety of
useful measures:

-   *Precision* [$TP/(TP + FP)$] is defined as the proportion of predicted positives
    that are actually positive. Also called *positive predictive value*. Answers the
    question: "Of all the flowers the model predicted to be *setosa*, what fraction
    actually were?"

-   *Recall* [$TP/(TP + FN)$] is defined as the proportion of positive results out of
    the number of samples which were actually positive. Also called *sensitivity*.
    Answers the question: "Of all the flowers that are actually *setosa*, what fraction
    did the model identify?"

-   *Specificity* [$TN/(TN + FP)$] is defined as the proportion of negative results out
    of the number of samples which were actually negative. Answers the question: "Of all
    flowers that were **not** *setosa*, what fraction did the model identify?"

-   *Accuracy* [$(TP + TN)/(TP + TN + FP + FN)$] is the percentage of labels predicted
    accurately for a sample. Answers the question: "Of all the observations, what
    fraction were correctly classified?"

-   *F Measure* is a weighted average of the precision and recall, with best 1 and worst
    being 0.

-   *Cohen's Kappa* has other uses, but when determining the performance of a
    classification model it provides an estimate of how much better the observed
    accuracy (calculated as shown above) is than the expected accuracy. For example, if
    the expected accuracy is 50% (random chance) and the observed accuracy is 95%,
    $\kappa$ will be 0.90. This is especially useful when the class distribution is
    skewed. Kappa can be calculated as shown:

    $$\begin{align}
    observations &= n = TP + TN + FP + FN \\
    accuracy_{obs} &= \frac{TP + TN}{n}\\
    accuracy_{exp} &= \left(\frac{TP * FP}{obs} + \frac{TN * FN}{obs}\right) \div n\\
    \kappa &= \frac{accuracy_{obs} - accuracy{exp}}{1 - accuracy_{exp}}
    \end{align}$$

As described in [How it Works](#ch5-how-it-works), a logistic regression model doesn't
*exactly* predict the class of each observation, but a set of probabilities that the
observation belongs to each class. In the binary case, these are the probability that
the observed class is 'true' (\$p(X)\$) and the probability that it is 'false' (\$1 -
p(X)\$). By default, if $p(X) > 0.5$, then the predicted class will be 'true'. This
threshold can be manipulated in order to further evaluate the model fit. By plotting the
*sensitivity* against [1 - *specificity*] for a range of threshold values, you get a
*received operator characteristic* (ROC) chart:

```{r}
(iris_predictions
  |> roc_curve(truth = setosa, .pred_TRUE) 
  |> autoplot())
```

The dotted diagonal line represents the probability of randomly guessing the correct
class, so you want to be as far from that line as possible! For a theoretical model
making perfect predictions, the curve would rise straight up the left side then across
the top. The *area under the curve* (AUC) is a value between 0 and 1 that provides a
quantitative measurement of the performance indicated by the ROC curve. The closer this
value is to 1, the better the model has performed.

```{r}
# Define a set of metrics using the `yardstick` package
eval_metrics <- metric_set(ppv,    recall, specificity, accuracy, 
                           f_meas, kap,    roc_auc)
eval_metrics(
  data = iris_predictions, 
  truth = setosa, 
  estimate = .pred_class,
  .pred_TRUE  # to be passed to `roc_auc()`
)
```

### Multinomial Logistic Regression {#ch5-multinomial-logistic-regression}

When expanding our predictive value to predicting *all* classes of iris species:

```{r include=FALSE}
# This recipe assumes that `Species` is predicted by all other values,
# creates interaction terms, and normalizes all the numeric predictors.
(iris_recipe
  <- recipe(Species ~ ., data = iris_data)
  |> step_interact(Species ~ Sepal.Length:Sepal.Width)
  |> step_interact(Species ~ Petal.Length:Petal.Width)
  |> step_normalize(all_numeric_predictors()))

# Specify the model
(iris_model
  <- multinom_reg()      # Since there are three possible classes
  |> set_engine("nnet")  # Default for `multinom_reg`
  |> set_mode("classification"))

# Bundle into a workflow (with fit)
(iris_workflow
  <- workflow()
  |> add_recipe(iris_recipe)
  |> add_model(iris_model)
  |> fit(data = iris_data))

# Add predictions to the input data
iris_predictions <- augment(iris_workflow, iris_data)

# Create a confusion matrix (table)
(confusion_matrix
  <- iris_predictions
  |> count(Species, .pred_class)
  |> pivot_wider(names_from = .pred_class, values_from = n))
```

Here we see that our relatively "un-tuned" model does a good job of identifying *setosa*
species but struggles a bit more with *versicolor* and *virginica* (due in large part to
our 'tweaks' to the data set). Each row represents the true class of the flower while
each column represents the predicted class of each flower. In a perfect world, we would
only have numbers on the diagonal. We can examine the same metrics as we did in the
binomial case:

```{r}
eval_metrics <- metric_set(ppv,    recall, specificity, accuracy, 
                           f_meas, kap,    roc_auc)
eval_metrics(
  data = iris_predictions, 
  truth = Species, 
  estimate = .pred_class, 
  .pred_setosa,      # to be passed to `roc_auc()`
  .pred_versicolor,  # / /
  .pred_virginica    # /
)
```

## Example {#ch5-example}

*This section is currently under construction*
